{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version \"m4_v5\" is with an input of 9 variables where we removed the chi2 of the Kaon and the instentanious intensity Lambda. We will start by training a (9,530,200,1) FCNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load m4_v3.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Thu Aug 23 10:46:07 2018\n",
    "\n",
    "@author: jcarmignani\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jun 23 20:42:37 2018\n",
    "\n",
    "@author: J_Carmignani\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "\n",
    "\n",
    "#reg_utils-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "                    \n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    #m = X.shape[1] # number of examples\n",
    "    #layers_dims = [4, 10, 3, 1]\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = (np.random.randn(layer_dims[l], layer_dims[l-1]))*(np.sqrt(2./ (layer_dims[l-1])))\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    #assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n",
    "    #assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 256, seed = 0):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1] # number of training examples\n",
    "    mini_batches = []\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(math.floor(m/mini_batch_size)) # number of mini batchesof size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "# Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "\n",
    "        mini_batch_X = shuffled_X [:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y [:, num_complete_minibatches * mini_batch_size : m]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the loss) presented in Figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape ()\n",
    "                    b1 -- bias vector of shape ()\n",
    "                    W2 -- weight matrix of shape ()\n",
    "                    b2 -- bias vector of shape ()\n",
    "                    W3 -- weight matrix of shape ()\n",
    "                    b3 -- bias vector of shape ()\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the loss function (vanilla logistic loss)\n",
    "    \"\"\"\n",
    "        \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)+1e-8\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache\n",
    "\n",
    "def compute_cost(A3, Y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "    \n",
    "    Arguments:\n",
    "    a3 -- post-activation, output of forward propagation\n",
    "    Y -- \"true\" labels vector, same shape as a3\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the cost function\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    a=np.reshape(A3,(1,m))\n",
    "    #logprobs = (np.multiply(-(np.log(a)),Y) + np.multiply(-(np.log(1 - a)),(1 - Y)))\n",
    "    cost = (np.nansum((np.multiply(-(np.log(a)),Y) + np.multiply(-(np.log(1 - a)),(1 - Y)))))*(1./m)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def backward_propagation(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 =(1./m) * (A3 - Y)\n",
    "    dW3 = np.dot(dZ3, A2.T)\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.float64(A2 > 0))\n",
    "    dW2 = np.dot(dZ2, A1.T)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.float64(A1 > 0))\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def initialize_adam(parameters) :\n",
    "   \"\"\"\n",
    "   Initializes v and s as two python dictionaries with:\n",
    "   - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n",
    "   - values: numpy arrays of zeros of the same shape as the corresponding\n",
    "   gradients/parameters.\n",
    "   Arguments:\n",
    "   parameters -- python dictionary containing your parameters.\n",
    "   parameters[\"W\" + str(l)] = Wl\n",
    "   parameters[\"b\" + str(l)] = bl\n",
    "   Returns:\n",
    "   v -- python dictionary that will contain the exponentially weighted average of the\n",
    "   gradient.\n",
    "   v[\"dW\" + str(l)] = ...\n",
    "   v[\"db\" + str(l)] = ...\n",
    "   s -- python dictionary that will contain the exponentially weighted average of the\n",
    "   squared gradient.\n",
    "   s[\"dW\" + str(l)] = ...\n",
    "   s[\"db\" + str(l)] = ...\n",
    "   \"\"\"\n",
    "\n",
    "   L = len(parameters) // 2 # number of layers in the neural networks\n",
    "   v = {}\n",
    "   s = {}\n",
    "# Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "   for l in range(L):\n",
    "### START CODE HERE ### (approx. 4 lines)\n",
    "       v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
    "       v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
    "       s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
    "       s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
    "### END CODE HERE ###\n",
    "   return v, s\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 1e-7,\n",
    "beta1 = 0.9, beta2 = 0.99, epsilon = 1e-8):\n",
    "   \"\"\"\n",
    "   Update parameters using Adam\n",
    "   Arguments:\n",
    "   parameters -- python dictionary containing your parameters:\n",
    "   parameters['W' + str(l)] = Wl\n",
    "   parameters['b' + str(l)] = bl\n",
    "   grads -- python dictionary containing your gradients for each parameters:\n",
    "   grads['dW' + str(l)] = dWl\n",
    "   grads['db' + str(l)] = dbl\n",
    "   v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "   s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "   learning_rate -- the learning rate, scalar.\n",
    "   beta1 -- Exponential decay hyperparameter for the first moment estimates\n",
    "   beta2 -- Exponential decay hyperparameter for the second moment estimates\n",
    "   epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "   Returns:\n",
    "   parameters -- python dictionary containing your updated parameters\n",
    "   v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "   s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "   \"\"\"\n",
    "   L = len(parameters) // 2 # number of layers in the neural networks\n",
    "   v_corrected = {} # Initializing first moment estimate, python dictionary\n",
    "   s_corrected = {} # Initializing second moment estimate, python dictionary\n",
    "# Perform Adam update on all parameters\n",
    "   for l in range(L):\n",
    "# Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "      v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*grads[\"dW\" + str(l+1)]\n",
    "      v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*grads[\"db\" + str(l+1)]\n",
    "      v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-beta1**t)\n",
    "      v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-beta1**t)\n",
    "      s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*((grads[\"dW\" + str(l+1)])**2)\n",
    "      s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*((grads[\"db\" + str(l+1)])**2)\n",
    "      s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-beta2**t)\n",
    "      s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-beta2**t)\n",
    "      parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*v_corrected[\"dW\" + str(l+1)]/((s_corrected[\"dW\" + str(l+1)])**0.5 + epsilon)\n",
    "      parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*v_corrected[\"db\" + str(l+1)]/((s_corrected[\"db\" + str(l+1)])**0.5 + epsilon)\n",
    "       \n",
    "   return parameters, v, s\n",
    "\n",
    "   \n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m), dtype = np.int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    A3, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    file1=open(\"misspredictions_m4v5_Kaon.txt\",\"w\")\n",
    "    file2=open(\"misspredictions_m4v5_Pileup.txt\",\"w\")\n",
    "\n",
    "    for i in range(0, A3.shape[1]):\n",
    "        if A3[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    # print/save results\n",
    "    for j in range(0, A3.shape[1]):\n",
    "        if p[0,j] == y[0,j]:\n",
    "            continue\n",
    "        else:\n",
    "            if p[0,j] == 1:\n",
    "                file1.write(str(j)+ \" \" + str(p[0,j])+\"\\n\")\n",
    "            else:\n",
    "                file2.write(str(j)+ \" \" + str(p[0,j])+\"\\n\")\n",
    "    file1.close()\n",
    "    file2.close()\n",
    "    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n",
    "    \n",
    "    return p\n",
    "    \n",
    "\n",
    "\n",
    "def predict_dec(parameters, X):\n",
    "    \"\"\"\n",
    "    Used for plotting decision boundary.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (m, K)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predict using forward propagation and a classification threshold of 0.5\n",
    "    A3, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A3>0.5)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \n",
    "  #plt.scatter(train_X[2, :], train_X[3, :], c=train_Y, s=40, cmap=plt.cm.Spectral);\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#11vars-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "tchod1= np.genfromtxt(\"tCHODS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "tchod1_test= np.genfromtxt(\"tCHODS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "trich1= np.genfromtxt(\"tRICHS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "trich1_test= np.genfromtxt(\"tRICHS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "tgtk1= np.genfromtxt(\"tGTKS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "tgtk1_test= np.genfromtxt(\"tGTKS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "tktag1= np.genfromtxt(\"tKTAGS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "tktag1_test= np.genfromtxt(\"tKTAGS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "cda1= np.genfromtxt(\"CDAS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "cda1_test= np.genfromtxt(\"CDAS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "chi21= np.genfromtxt(\"CHI2S1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "chi21_test= np.genfromtxt(\"CHI2S1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "chi2p1= np.genfromtxt(\"CHI2PS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "chi2p1_test= np.genfromtxt(\"CHI2PS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "lamd1= np.genfromtxt(\"LAMBDAS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "lamd1_test= np.genfromtxt(\"LAMBDAS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "dx1= np.genfromtxt(\"DXS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "dx1_test= np.genfromtxt(\"DXS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "dy1= np.genfromtxt(\"DYS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "dy1_test= np.genfromtxt(\"DYS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "vtrx1= np.genfromtxt(\"VTRXS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "vtrx1_test= np.genfromtxt(\"VTRXS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "vtry1= np.genfromtxt(\"VTRYS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "vtry1_test= np.genfromtxt(\"VTRYS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "tchod0= np.genfromtxt(\"tCHODB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "tchod0_test= np.genfromtxt(\"tCHODB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "trich0= np.genfromtxt(\"tRICHB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "trich0_test= np.genfromtxt(\"tRICHB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "tgtk0= np.genfromtxt(\"tGTKB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "tgtk0_test= np.genfromtxt(\"tGTKB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "tktag0= np.genfromtxt(\"tKTAGB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "tktag0_test= np.genfromtxt(\"tKTAGB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "cda0= np.genfromtxt(\"CDAB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "cda0_test= np.genfromtxt(\"CDAB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "chi20= np.genfromtxt(\"CHI2B1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "chi20_test= np.genfromtxt(\"CHI2B1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "chi2p0= np.genfromtxt(\"CHI2PB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "chi2p0_test= np.genfromtxt(\"CHI2PB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "lamd0= np.genfromtxt(\"LAMBDAB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "lamd0_test= np.genfromtxt(\"LAMBDAB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "dx0= np.genfromtxt(\"DXB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "dx0_test= np.genfromtxt(\"DXB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "dy0= np.genfromtxt(\"DYB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "dy0_test= np.genfromtxt(\"DYB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "vtrx0= np.genfromtxt(\"VTRXB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "vtrx0_test= np.genfromtxt(\"VTRXB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "vtry0= np.genfromtxt(\"VTRYB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "vtry0_test= np.genfromtxt(\"VTRYB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "deltatchod1=tchod1-tgtk1\n",
    "deltatchod1_test=tchod1_test-tgtk1_test\n",
    "deltatrich1=trich1-tgtk1\n",
    "deltatrich1_test=trich1_test-tgtk1_test\n",
    "deltatchod0=tchod0-tgtk0\n",
    "deltatchod0_test=tchod0_test-tgtk0_test\n",
    "deltatrich0=trich0-tgtk0\n",
    "deltatrich0_test=trich0_test-tgtk0_test\n",
    "deltatktag0=tktag0-tgtk0\n",
    "deltatktag0_test=tktag0_test-tgtk0_test\n",
    "deltatktag1=tktag1-tgtk1\n",
    "deltatktag1_test=tktag1_test-tgtk1_test\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "deltatrich=np.hstack((deltatrich0,deltatrich1))\n",
    "deltatchod=np.hstack((deltatchod0,deltatchod1))\n",
    "deltatktag=np.hstack((deltatktag0,deltatktag1))\n",
    "deltatrich_test=np.hstack((deltatrich0_test,deltatrich1_test)) \n",
    "deltatchod_test=np.hstack((deltatchod0_test,deltatchod1_test)) \n",
    "deltatktag_test=np.hstack((deltatktag0_test,deltatktag1_test)) \n",
    "cda=np.hstack((cda0,cda1))\n",
    "chi2=np.hstack((chi20,chi21))\n",
    "chi2p=np.hstack((chi2p0,chi2p1))\n",
    "lamd=np.hstack((lamd0,lamd1))\n",
    "dx=np.hstack((dx0,dx1))\n",
    "dy=np.hstack((dy0,dy1))\n",
    "vtrx=np.hstack((vtrx0,vtrx1))\n",
    "vtry=np.hstack((vtry0,vtry1))\n",
    "cda_test=np.hstack((cda0_test,cda1_test))\n",
    "chi2_test=np.hstack((chi20_test,chi21_test))\n",
    "chi2p_test=np.hstack((chi2p0_test,chi2p1_test))\n",
    "lamd_test=np.hstack((lamd0_test,lamd1_test))\n",
    "dx_test=np.hstack((dx0_test,dx1_test))\n",
    "dy_test=np.hstack((dy0_test,dy1_test))\n",
    "vtrx_test=np.hstack((vtrx0_test,vtrx1_test))\n",
    "vtry_test=np.hstack((vtry0_test,vtry1_test))\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#For Background train\n",
    "mask00= (-1< deltatchod0) & (deltatchod0<1)\n",
    "x00a=deltatchod0[mask00]\n",
    "mask10= (-1< deltatrich0) & (deltatrich0<1)\n",
    "x10=deltatrich0[mask10]\n",
    "mask20= (-1< deltatktag0) & (deltatktag0<1)\n",
    "x20a=deltatktag0[mask20]\n",
    "mask30a= (cda0<15)\n",
    "x30a=cda0[mask30a]\n",
    "\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#'' x00a.shape                                                                    ''\n",
    "#'' x10.shape                                                                     ''\n",
    "#'' x20a.shape                                                                    ''\n",
    "#'' x30a.shape#to check the dimentionality then take the lowest dim mask for all  ''\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "x00=deltatchod0[mask10]\n",
    "x20=deltatktag0[mask10]# to fix the cut having the same shape\n",
    "x30=cda0[mask10]\n",
    "mask30=(x30<15)#fix the condition for mask30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x330=x30[mask30]#if lower dim apply it again for all\n",
    "x220=x20[mask30]\n",
    "x110=x10[mask30]\n",
    "x000=x00[mask30]\n",
    "#continue the same for all the other variables to unify the dimensions\n",
    "x40=chi20[mask10]\n",
    "x440=x40[mask30]\n",
    "x50=chi2p0[mask10]\n",
    "x550=x50[mask30]\n",
    "x60=lamd0[mask10]\n",
    "x660=x60[mask30]\n",
    "x70=dx0[mask10]\n",
    "x770=x70[mask30]\n",
    "x80=dy0[mask10]\n",
    "x880=x80[mask30]\n",
    "x90=vtrx0[mask10]\n",
    "x990=x90[mask30]\n",
    "y90=vtry0[mask10]\n",
    "y990=y90[mask30]\n",
    "\n",
    "nx000=(x000)#-min(x000))/(max(x000)-min(x000))\n",
    "nx110=(x110)#-min(x110))/(max(x110)-min(x110))\n",
    "nx220=(x220)#-min(x220))/(max(x220)-min(x220))\n",
    "nx330=(x330)#-min(x330))/(max(x330)-min(x330))\n",
    "nx440=(x440)#-min(x440))/(max(x440)-min(x440))\n",
    "nx550=(x550)#-min(x550))/(max(x550)-min(x550))\n",
    "nx660=(x660)#-min(x660))/(max(x660)-min(x660))\n",
    "nx770=(x770)#-min(x770))/(max(x770)-min(x770))\n",
    "nx880=(x880)#-min(x880))/(max(x880)-min(x880))\n",
    "nx990=(x990)#-min(x990))/(max(x990)-min(x990))\n",
    "ny990=(y990)#-min(y990))/(max(y990)-min(y990))\n",
    "\n",
    "\n",
    "z00=np.reshape(nx000,(1,314034))\n",
    "z10=np.reshape(nx110,(1,314034))\n",
    "z20=np.reshape(nx220,(1,314034))\n",
    "z30=np.reshape(nx330,(1,314034))\n",
    "#z40=np.reshape(nx440,(1,314034))\n",
    "z50=np.reshape(nx550,(1,314034))\n",
    "#z60=np.reshape(nx660,(1,314034))\n",
    "z70=np.reshape(nx770,(1,314034))\n",
    "z80=np.reshape(nx880,(1,314034))\n",
    "zx90=np.reshape(nx990,(1,314034))\n",
    "zy90=np.reshape(ny990,(1,314034))\n",
    "\n",
    "\n",
    "X0 = np.vstack((z00,z10,z20,z30,z50,z70,z80,zx90,zy90))\n",
    "Y0=np.zeros((1,X0.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#For Background 0 test\n",
    "mask00_test= (-1< deltatchod0_test) & (deltatchod0_test<1)\n",
    "x00a_test=deltatchod0_test[mask00_test]\n",
    "mask10_test= (-1< deltatrich0_test) & (deltatrich0_test<1)\n",
    "x10_test=deltatrich0_test[mask10_test]\n",
    "mask20_test= (-1< deltatktag0_test) & (deltatktag0_test<1)\n",
    "x20a_test=deltatktag0_test[mask20_test]\n",
    "mask30a_test= (cda0_test<15)\n",
    "x30a_test=cda0_test[mask30a_test]\n",
    "\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#'' x00a_test.shape                                                                    ''\n",
    "#'' x10_test.shape                                                                     ''\n",
    "#'' x20a_test.shape                                                                    ''\n",
    "#'' x30a_test.shape#to check the dimentionality then take the lowest dim mask for all  ''\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "x00_test=deltatchod0_test[mask10_test]\n",
    "x20_test=deltatktag0_test[mask10_test]# to fix the cut having the same shape\n",
    "x30_test=cda0_test[mask10_test]\n",
    "mask30_test=(x30_test<15)#fix the condition for mask30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x330_test=x30_test[mask30_test]#if lower dim apply it again for all\n",
    "x220_test=x20_test[mask30_test]\n",
    "x110_test=x10_test[mask30_test]\n",
    "x000_test=x00_test[mask30_test]\n",
    "#continue the same for all the other variables to unify the dimensions\n",
    "x40_test=chi20_test[mask10_test]\n",
    "x440_test=x40_test[mask30_test]\n",
    "x50_test=chi2p0_test[mask10_test]\n",
    "x550_test=x50_test[mask30_test]\n",
    "x60_test=lamd0_test[mask10_test]\n",
    "x660_test=x60_test[mask30_test]\n",
    "x70_test=dx0_test[mask10_test]\n",
    "x770_test=x70_test[mask30_test]\n",
    "x80_test=dy0_test[mask10_test]\n",
    "x880_test=x80_test[mask30_test]\n",
    "x90_test=vtrx0_test[mask10_test]\n",
    "x990_test=x90_test[mask30_test]\n",
    "y90_test=vtry0_test[mask10_test]\n",
    "y990_test=y90_test[mask30_test]\n",
    "\n",
    "nx000_test=(x000_test)#-min(x000_test))/(max(x000_test)-min(x000_test))\n",
    "nx110_test=(x110_test)#-min(x110_test))/(max(x110_test)-min(x110_test))\n",
    "nx220_test=(x220_test)#-min(x220_test))/(max(x220_test)-min(x220_test))\n",
    "nx330_test=(x330_test)#-min(x330_test))/(max(x330_test)-min(x330_test))\n",
    "nx440_test=(x440_test)#-min(x440_test))/(max(x440_test)-min(x440_test))\n",
    "nx550_test=(x550_test)#-min(x550_test))/(max(x550_test)-min(x550_test))\n",
    "nx660_test=(x660_test)#-min(x660_test))/(max(x660_test)-min(x660_test))\n",
    "nx770_test=(x770_test)#-min(x770_test))/(max(x770_test)-min(x770_test))\n",
    "nx880_test=(x880_test)#-min(x880_test))/(max(x880_test)-min(x880_test))\n",
    "nx990_test=(x990_test)#-min(x990_test))/(max(x990_test)-min(x990_test))\n",
    "ny990_test=(y990_test)#-min(y990_test))/(max(y990_test)-min(y990_test))\n",
    "\n",
    "\n",
    "z00_test=np.reshape(nx000_test,(1,35001))\n",
    "z10_test=np.reshape(nx110_test,(1,35001))\n",
    "z20_test=np.reshape(nx220_test,(1,35001))\n",
    "z30_test=np.reshape(nx330_test,(1,35001))\n",
    "#z40_test=np.reshape(nx440_test,(1,35001))\n",
    "z50_test=np.reshape(nx550_test,(1,35001))\n",
    "#z60_test=np.reshape(nx660_test,(1,35001))\n",
    "z70_test=np.reshape(nx770_test,(1,35001))\n",
    "z80_test=np.reshape(nx880_test,(1,35001))\n",
    "zx90_test=np.reshape(nx990_test,(1,35001))\n",
    "zy90_test=np.reshape(ny990_test,(1,35001))\n",
    "\n",
    "\n",
    "X0_test = np.vstack((z00_test,z10_test,z20_test,z30_test,z50_test,z70_test,z80_test,zx90_test,zy90_test))\n",
    "Y0_test=np.zeros((1,X0_test.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#For Signal 1 train\n",
    "mask01= (-1< deltatchod1) & (deltatchod1<1)\n",
    "x01a=deltatchod1[mask01]\n",
    "mask11= (-1< deltatrich1) & (deltatrich1<1)\n",
    "x11=deltatrich1[mask11]\n",
    "mask21= (-1< deltatktag1) & (deltatktag1<1)\n",
    "x21a=deltatktag1[mask21]\n",
    "mask31a= (cda1<15)\n",
    "x31a=cda1[mask31a]\n",
    "\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#'' x01a.shape                                                                    ''\n",
    "#'' x11.shape                                                                     ''\n",
    "#'' x21a.shape                                                                    ''\n",
    "#'' x31a.shape#to check the dimentionality then take the lowest dim mask for all  ''\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "x01=deltatchod1[mask11]\n",
    "x21=deltatktag1[mask11]# to fix the cut having the same shape\n",
    "x31=cda1[mask11]\n",
    "mask31=(x31<15)#fix the condition for mask30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x331=x31[mask31]#if lower dim apply it again for all\n",
    "x221=x21[mask31]\n",
    "x111=x11[mask31]\n",
    "x001=x01[mask31]\n",
    "#continue the same for all the other variables to unify the dimensions\n",
    "x41=chi21[mask11]\n",
    "x441=x41[mask31]\n",
    "x51=chi2p1[mask11]\n",
    "x551=x51[mask31]\n",
    "x61=lamd1[mask11]\n",
    "x661=x61[mask31]\n",
    "x71=dx1[mask11]\n",
    "x771=x71[mask31]\n",
    "x81=dy1[mask11]\n",
    "x881=x81[mask31]\n",
    "x91=vtrx1[mask11]\n",
    "x991=x91[mask31]\n",
    "y91=vtry1[mask11]\n",
    "y991=y91[mask31]\n",
    "\n",
    "nx001=(x001)#-min(x001))/(max(x001)-min(x001))\n",
    "nx111=(x111)#-min(x111))/(max(x111)-min(x111))\n",
    "nx221=(x221)#-min(x221))/(max(x221)-min(x221))\n",
    "nx331=(x331)#-min(x331))/(max(x331)-min(x331))\n",
    "nx441=(x441)#-min(x441))/(max(x441)-min(x441))\n",
    "nx551=(x551)#-min(x551))/(max(x551)-min(x551))\n",
    "nx661=(x661)#-min(x661))/(max(x661)-min(x661))\n",
    "nx771=(x771)#-min(x771))/(max(x771)-min(x771))\n",
    "nx881=(x881)#-min(x881))/(max(x881)-min(x881))\n",
    "nx991=(x991)#-min(x991))/(max(x991)-min(x991))\n",
    "ny991=(y991)#-min(y991))/(max(y991)-min(y991))\n",
    "\n",
    "\n",
    "z01=np.reshape(nx001,(1,417049))\n",
    "z11=np.reshape(nx111,(1,417049))\n",
    "z21=np.reshape(nx221,(1,417049))\n",
    "z31=np.reshape(nx331,(1,417049))\n",
    "#z41=np.reshape(nx441,(1,417049))\n",
    "z51=np.reshape(nx551,(1,417049))\n",
    "#z61=np.reshape(nx661,(1,417049))\n",
    "z71=np.reshape(nx771,(1,417049))\n",
    "z81=np.reshape(nx881,(1,417049))\n",
    "zx91=np.reshape(nx991,(1,417049))\n",
    "zy91=np.reshape(ny991,(1,417049))\n",
    "\n",
    "\n",
    "X1 = np.vstack((z01,z11,z21,z31,z51,z71,z81,zx91,zy91))\n",
    "Y1=np.ones((1,X1.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#For Signal 1 test\n",
    "mask01_test= (-1< deltatchod1_test) & (deltatchod1_test<1)\n",
    "x01a_test=deltatchod1_test[mask01_test]\n",
    "mask11_test= (-1< deltatrich1_test) & (deltatrich1_test<1)\n",
    "x11_test=deltatrich1_test[mask11_test]\n",
    "mask21_test= (-1< deltatktag1_test) & (deltatktag1_test<1)\n",
    "x21a_test=deltatktag1_test[mask21_test]\n",
    "mask31a_test= (cda1_test<15)\n",
    "x31a_test=cda1_test[mask31a_test]\n",
    "\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#'' x01a_test.shape                                                                    ''\n",
    "#'' x11_test.shape                                                                     ''\n",
    "#'' x21a_test.shape                                                                    ''\n",
    "#'' x31a_test.shape#to check the dimentionality then take the lowest dim mask for all  ''\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "x01_test=deltatchod1_test[mask11_test]\n",
    "x21_test=deltatktag1_test[mask11_test]# to fix the cut having the same shape\n",
    "x31_test=cda1_test[mask11_test]\n",
    "mask31_test=(x31_test<15)#fix the condition for mask30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x331_test=x31_test[mask31_test]#if lower dim apply it again for all\n",
    "x221_test=x21_test[mask31_test]\n",
    "x111_test=x11_test[mask31_test]\n",
    "x001_test=x01_test[mask31_test]\n",
    "#continue the same for all the other variables to unify the dimensions\n",
    "x41_test=chi21_test[mask11_test]\n",
    "x441_test=x41_test[mask31_test]\n",
    "x51_test=chi2p1_test[mask11_test]\n",
    "x551_test=x51_test[mask31_test]\n",
    "x61_test=lamd1_test[mask11_test]\n",
    "x661_test=x61_test[mask31_test]\n",
    "x71_test=dx1_test[mask11_test]\n",
    "x771_test=x71_test[mask31_test]\n",
    "x81_test=dy1_test[mask11_test]\n",
    "x881_test=x81_test[mask31_test]\n",
    "x91_test=vtrx1_test[mask11_test]\n",
    "x991_test=x91_test[mask31_test]\n",
    "y91_test=vtry1_test[mask11_test]\n",
    "y991_test=y91_test[mask31_test]\n",
    "\n",
    "\n",
    "nx001_test=(x001_test)#-min(x001_test))/(max(x001_test)-min(x001_test))\n",
    "nx111_test=(x111_test)#-min(x111_test))/(max(x111_test)-min(x111_test))\n",
    "nx221_test=(x221_test)#-min(x221_test))/(max(x221_test)-min(x221_test))\n",
    "nx331_test=(x331_test)#-min(x331_test))/(max(x331_test)-min(x331_test))\n",
    "nx441_test=(x441_test)#-min(x441_test))/(max(x441_test)-min(x441_test))\n",
    "nx551_test=(x551_test)#-min(x551_test))/(max(x551_test)-min(x551_test))\n",
    "nx661_test=(x661_test)#-min(x661_test))/(max(x661_test)-min(x661_test))\n",
    "nx771_test=(x771_test)#-min(x771_test))/(max(x771_test)-min(x771_test))\n",
    "nx881_test=(x881_test)#-min(x881_test))/(max(x881_test)-min(x881_test))\n",
    "nx991_test=(x991_test)#-min(x991_test))/(max(x991_test)-min(x991_test))\n",
    "ny991_test=(y991_test)#-min(y991_test))/(max(y991_test)-min(y991_test))\n",
    "\n",
    "\n",
    "z01_test=np.reshape(nx001_test,(1,46359))\n",
    "z11_test=np.reshape(nx111_test,(1,46359))\n",
    "z21_test=np.reshape(x221_test,(1,46359))\n",
    "z31_test=np.reshape(nx331_test,(1,46359))\n",
    "#z41_test=np.reshape(nx441_test,(1,46359))\n",
    "z51_test=np.reshape(nx551_test,(1,46359))\n",
    "#z61_test=np.reshape(nx661_test,(1,46359))\n",
    "z71_test=np.reshape(nx771_test,(1,46359))\n",
    "z81_test=np.reshape(nx881_test,(1,46359))\n",
    "zx91_test=np.reshape(nx991_test,(1,46359))\n",
    "zy91_test=np.reshape(ny991_test,(1,46359))\n",
    "\n",
    "\n",
    "X1_test = np.vstack((z01_test,z11_test,z21_test,z31_test,z51_test,z71_test,z81_test,zx91_test,zy91_test))\n",
    "Y1_test = np.ones((1,X1_test.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "#input data X/output tag Y\n",
    "\n",
    "X=np.hstack((X0,X1))\n",
    "Y=np.hstack((Y0,Y1))\n",
    "\n",
    "X_test=np.hstack((X0_test,X1_test))\n",
    "Y_test=np.hstack((Y0_test,Y1_test))\n",
    "\n",
    "\n",
    "#X.shape\n",
    "#Y.shape\n",
    "#X_test.shape\n",
    "#Y_test.shape\n",
    "\n",
    "col_idx = np.random.permutation(X.shape[1])\n",
    "shuffled_X = X[:,col_idx]\n",
    "shuffled_Y = Y[:,col_idx]\n",
    "\n",
    "col_idx_test= np.random.permutation(X_test.shape[1])\n",
    "shuffled_X_test = X_test[:,col_idx_test]\n",
    "shuffled_Y_test = Y_test[:,col_idx_test]\n",
    "\n",
    "\n",
    "%matplotlib  inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "\n",
    "def model(X, Y, layers_dims, learning_rate = 1e-7, mini_batch_size = 256, beta1 = 0.9, beta2 = 0.99, epsilon = 1e-8, num_epochs = 7700, print_cost =True):\n",
    "    \n",
    "    L = len(layers_dims) # number of layers in the neural networks\n",
    "    costs = [] # to keep track of the cost\n",
    "    t = 0\n",
    "    seed = 10\n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    v, s = initialize_adam(parameters)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        for minibatch in minibatches:\n",
    "# Select a minibatch\n",
    "           (minibatch_X, minibatch_Y) = minibatch\n",
    "# Forward propagation\n",
    "           a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "# Compute cost\n",
    "           cost = compute_cost(a3, minibatch_Y)\n",
    "# Backward propagation\n",
    "           grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "           t = t + 1 # Adam counter\n",
    "           parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,t, learning_rate, beta1, beta2, epsilon)\n",
    "           \n",
    "    # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 100 == 0 :\n",
    "           print (\"Cost after epoch %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10 == 0 :\n",
    "           costs.append(cost)\n",
    "       \n",
    "       \n",
    " # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "    return parameters   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/na62/jcarmignani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:174: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.251163\n",
      "Cost after epoch 100: 0.447985\n",
      "Cost after epoch 200: 0.268508\n",
      "Cost after epoch 300: 0.232861\n",
      "Cost after epoch 400: 0.195245\n",
      "Cost after epoch 500: 0.205239\n",
      "Cost after epoch 600: 0.196367\n",
      "Cost after epoch 700: 0.193654\n",
      "Cost after epoch 800: 0.130549\n",
      "Cost after epoch 900: 0.190544\n",
      "Cost after epoch 1000: 0.206335\n",
      "Cost after epoch 1100: 0.153200\n",
      "Cost after epoch 1200: 0.158590\n",
      "Cost after epoch 1300: 0.144160\n",
      "Cost after epoch 1400: 0.153322\n",
      "Cost after epoch 1500: 0.187334\n",
      "Cost after epoch 1600: 0.180980\n",
      "Cost after epoch 1700: 0.218246\n",
      "Cost after epoch 1800: 0.155851\n",
      "Cost after epoch 1900: 0.217495\n",
      "Cost after epoch 2000: 0.194506\n",
      "Cost after epoch 2100: 0.156758\n",
      "Cost after epoch 2200: 0.162977\n",
      "Cost after epoch 2300: 0.162828\n",
      "Cost after epoch 2400: 0.161526\n",
      "Cost after epoch 2500: 0.124581\n",
      "Cost after epoch 2600: 0.175831\n",
      "Cost after epoch 2700: 0.157145\n",
      "Cost after epoch 2800: 0.184717\n",
      "Cost after epoch 2900: 0.106389\n",
      "Cost after epoch 3000: 0.171994\n",
      "Cost after epoch 3100: 0.188803\n",
      "Cost after epoch 3200: 0.232076\n",
      "Cost after epoch 3300: 0.202635\n",
      "Cost after epoch 3400: 0.097860\n",
      "Cost after epoch 3500: 0.142821\n",
      "Cost after epoch 3600: 0.158072\n",
      "Cost after epoch 3700: 0.162509\n",
      "Cost after epoch 3800: 0.185450\n",
      "Cost after epoch 3900: 0.202874\n",
      "Cost after epoch 4000: 0.144835\n",
      "Cost after epoch 4100: 0.194861\n",
      "Cost after epoch 4200: 0.213290\n",
      "Cost after epoch 4300: 0.174222\n",
      "Cost after epoch 4400: 0.187726\n",
      "Cost after epoch 4500: 0.145525\n",
      "Cost after epoch 4600: 0.186203\n",
      "Cost after epoch 4700: 0.199084\n",
      "Cost after epoch 4800: 0.130125\n",
      "Cost after epoch 4900: 0.167924\n",
      "Cost after epoch 5000: 0.167811\n",
      "Cost after epoch 5100: 0.160439\n",
      "Cost after epoch 5200: 0.136720\n",
      "Cost after epoch 5300: 0.182071\n",
      "Cost after epoch 5400: 0.133340\n",
      "Cost after epoch 5500: 0.187827\n",
      "Cost after epoch 5600: 0.167243\n",
      "Cost after epoch 5700: 0.179207\n",
      "Cost after epoch 5800: 0.226081\n",
      "Cost after epoch 5900: 0.109070\n",
      "Cost after epoch 6000: 0.161766\n",
      "Cost after epoch 6100: 0.121093\n",
      "Cost after epoch 6200: 0.130329\n",
      "Cost after epoch 6300: 0.152863\n",
      "Cost after epoch 6400: 0.187303\n",
      "Cost after epoch 6500: 0.257980\n",
      "Cost after epoch 6600: 0.128838\n",
      "Cost after epoch 6700: 0.137200\n",
      "Cost after epoch 6800: 0.169236\n",
      "Cost after epoch 6900: 0.126485\n",
      "Cost after epoch 7000: 0.174163\n",
      "Cost after epoch 7100: 0.240457\n",
      "Cost after epoch 7200: 0.178838\n",
      "Cost after epoch 7300: 0.215543\n",
      "Cost after epoch 7400: 0.191487\n",
      "Cost after epoch 7500: 0.244628\n",
      "Cost after epoch 7600: 0.234954\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAEWCAYAAAD2AJlUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd4W+X5//H37RU7ezl7OCEhAQIhEJJAIIQySthQNoUyWgqFL2X0x+hglkILpZRCGWXvvQmEESCQkD0gk+zEGc52hh3P5/fHOZIlWbKdIcvhfF7XpcvS0ZHOLevo3OcZ53nMOYeIiEiQpKU6ABERkfqm5CciIoGj5CciIoGj5CciIoGj5CciIoGj5CciIoGj5CeyC8zsYzP7VarjEJEdo+QneyQzW2Jmx6Q6DufcCOfcc6mOA8DMvjKzX6dgu1eb2WQzKzGzZ3fD+11nZqvNrNDMnjazRv7ybma2NebmzOyGXf4QEjhKfiIJmFlGqmMIaUixxLES+Cvw9K6+kZn9HLgZOBrIA3oCdwA455Y555qGbsD+QCXw1q5uV4JHyU9+cszsJDObbmabzGycmR0Q8dzNZrbQzLaY2WwzOz3iuYvNbKyZ/cvMNgC3+8u+NbP7zWyjmS02sxERrwmXtuqwbg8zG+Nv+3Mze8TMXkzwGYabWb6Z3WRmq4FnzKyVmX1oZmv99//QzLr4698NHAE87JeIHvaX9zWzz8xsg5nNM7Ozd+9/G5xzbzvn3gXWJ/gsCb+POH4FPOWcm+Wc2wjcBVycYN2LgDHOuSW7EL4ElJKf/KSY2UF4JZDfAm2Ax4H3Q1VnwEK8JNECr0Txopl1jHiLwcAioB1wd8SyeUBb4B/AU2ZmCUKoad2XgYl+XLcDF9bycToArYHuwOV4v9dn/MfdgGLgYQDn3J+Ab4Cr/ZLR1WbWBPjM32474Dzgv2a2X7yNmdl//QQV7/Z9LbHGVYfvI9Z+wIyIxzOA9mbWJs66FwENospZ9jxKfvJT8xvgcefcBOdchd8eVwIMAXDOveGcW+mcq3TOvQbMBwZFvH6lc+4/zrly51yxv2ypc+5/zrkKvINtR6B9gu3HXdfMugGHALc650qdc98C79fyWSqB25xzJc65YufceufcW865IufcFrzkfGQNrz8JWOKce8b/PFPxqgjPjLeyc+53zrmWCW41ldZqUuP3EUdToDDiceh+s8iVzOwIvO/gzZ2MSwJOyU9+aroDN0SWWoCuQCcAM7soogpuE9APr5QWsjzOe64O3XHOFfl3mybYfqJ1OwEbIpYl2laktc657aEHZtbYzB43s6VmthkYA7Q0s/QEr+8ODI75X1yAV6KsLwm/DzO7IKLjysf++luB5hGvD93fEvO+vwLecs5tTWr08pPVkBvRRXbGcuBu59zdsU+YWXfgf3idKb5zzlWY2XQgsgozWdOcrAJam1njiATYtZbXxMZyA9AHGOycW21mBwLTqIo/dv3lwNfOuWPrEqCZPQb8MsHTS51zcatLa5Hw+/C9FPN4FtAfeN1/3B8ocM6F2xPNLAc4CzgdkZ2kkp/syTLNLDviloGX3K4ws8HmaWJmJ5pZM6AJXoJYC2Bml+CV/JLOObcUmIzXiSbLzA4FTt7Bt2mG1863ycxaA7fFPF+A1zsy5ENgbzO70Mwy/dshZrZPghiviOxNGXNLmPjMLMPMsoF0ID3iu4Cav494ngcuM7N9zawV8Gfg2Zh1Tgc2AV8mikmkNkp+sicbiZcMQrfbnXOT8dqZHgY2Agvwews652YD/wS+w0sU+wNj6zHeC4BD8XpF/hV4Da/9q64eBHKAdcB44JOY5/8NnOn3BH3Ibxc8DjgX73KE1cDfgUSdTXbWn/H+/zfjlRyL/WXU9H3E45z7BK+j0JfAUv8Wm+R/BTzvNBmp7ALT/iOSGmb2GjDXORd7cBeRJFPJT6Se+FWOe5lZmpkdD5wKvJvquESCSB1eROpPB+BtvOvd8oErnXPTUhuSSDCp2lNERAJH1Z4iIhI4e1y1Z9u2bV1eXl6qwxARkQZkypQp65xzuXVdf49Lfnl5eUyePDnVYYiISANiZkt3ZH1Ve4qISOAo+YmISOAo+YmISOAo+YmISOAo+YmISOAo+YmISOAo+YmISOAEMvm9OH4p70zLT3UYIiKSIoFMfq9NWs7701emOgwREUmRQCY/s1RHICIiqRTI5AeguSxERIIrkMnPAM3kJCISXIFMfpip5CciEmCBTH5eyU/pT0QkqIKZ/NThRUQk0AKZ/EREJNgCmfzU4UVEJNiCmfzMcOryIiISWMFMfqkOQEREUiqQyQ9U7SkiEmSBTH5mSn4iIkEWzOSH2vxERIIskMlPjX4iIsEWyOSnSx1ERIItkMkPNKuDiEiQBTL5maHsJyISYMFMfurwIiISaMFMfurwIiISaElLfmbW1cy+NLM5ZjbLzH4fZx0zs4fMbIGZfW9mByUrnljq8CIiElwZSXzvcuAG59xUM2sGTDGzz5xzsyPWGQH09m+DgUf9v0llpiY/EZEgS1rJzzm3yjk31b+/BZgDdI5Z7VTgeecZD7Q0s47JiinEME1mKyISYPXS5mdmecAAYELMU52B5RGP86meIDGzy81ssplNXrt27W6IZ5ffQkRE9mBJT35m1hR4C7jWObc59uk4L6lWJHPOPeGcG+icG5ibm7tb4lK5T0QkuJKa/MwsEy/xveScezvOKvlA14jHXYCVyYwpRLWeIiLBlczengY8Bcxxzj2QYLX3gYv8Xp9DgELn3KpkxRQRm0p+IiIBlszenkOBC4EfzGy6v+yPQDcA59xjwEjgBGABUARcksR4wtTkJyISbElLfs65b6klzzivy+VVyYohEdPI1iIigRbIEV5AHV5ERIIskMlPBT8RkWALZvIzDWwtIhJkwUx+qQ5ARERSKpDJD1TtKSISZIFMfmZKfiIiQRbI5Ae6yF1EJMgCmfw0sLWISLAFM/mBpjQSEQmwQCY/EREJtkAmP3V4EREJtmAmP13pJyISaMFMfoZGeBERCbBAJj9QtaeISJAFMvl5JT8REQmqYCY/TJc6iIgEWCCTn/q7iIgEWyCTn6FqTxGRIAtk8gOU/UREAiyQyc+bzFZERIIqmMkv1QGIiEhKBTP5mQa2FhEJskAmP1CTn4hIkAUy+XlTGqU6ChERSZVgJj/NZisiEmjBTH5oYGsRkSALZPJD8/mJiARaMJMfSn4iIkEWyOSnyWxFRIItmMlPuU9EJNCCmfzQRe4iIkEWyOQHushdRCTIApn8TL09RUQCLZjJTx1eREQCLZjJz3SRu4hIkAU3+Sn3iYgEVtKSn5k9bWZrzGxmgueHm1mhmU33b7cmK5Z4lPtERIIrI4nv/SzwMPB8Det845w7KYkxJKA2PxGRIEtayc85NwbYkKz33xWq9hQRCbZUt/kdamYzzOxjM9uvvjbqlfuU/UREgiqZ1Z61mQp0d85tNbMTgHeB3vFWNLPLgcsBunXrtls2rpKfiEhwpazk55zb7Jzb6t8fCWSaWdsE6z7hnBvonBuYm5u7y9v2LnUQEZGgSlnyM7MO5k+pbmaD/FjW18u21eFFRCTQklbtaWavAMOBtmaWD9wGZAI45x4DzgSuNLNyoBg419XTaNNehxeV/UREgippyc85d14tzz+MdylEvTNU7SkiEmSp7u2ZMir4iYgEVyCTn2k2WxGRQAtk8gO1+YmIBFkgk58udRARCbZAJj9A2U9EJMACmfwMU+4TEQmwYCY/9XcREQm0YCY/1OFFRCTIgpn81OFFRCTQApn8QBe5i4gEWSCTny5yFxEJtmAmP8Cp4lNEJLACmfwwVXuKiARZMJMf6vAiIhJkgUx+msxWRCTYgpn8NKGfiEigBTP5oQ4vIiJBVqfkZ2Zn1WXZnsLU4UVEJNDqWvK7pY7L9hjKfSIiwZVR05NmNgI4AehsZg9FPNUcKE9mYMmkDi8iIsFWY/IDVgKTgVOAKRHLtwDXJSuoZPOqPVX2ExEJqhqTn3NuBjDDzF52zpUBmFkroKtzbmN9BJgM6uwpIhJsdW3z+8zMmptZa2AG8IyZPZDEuJJOBT8RkeCqa/Jr4ZzbDJwBPOOcOxg4JnlhJZkGthYRCbS6Jr8MM+sInA18mMR46oVSn4hIsNU1+d0JjAIWOucmmVlPYH7ywkquUMFPnV5ERIKptt6eADjn3gDeiHi8CPhFsoKqL86pBlREJIjqOsJLFzN7x8zWmFmBmb1lZl2SHVyy6Do/EZFgq2u15zPA+0AnoDPwgb9sjxSu9kxtGCIikiJ1TX65zrlnnHPl/u1ZIDeJcSVVqNynNj8RkWCqa/JbZ2a/NLN0//ZLYH0yA0smlfxERIKtrsnvUrzLHFYDq4AzgUuSFVR9UcFPRCSY6tTbE7gL+FVoSDN/pJf78ZLiHsfUxVNEJNDqWvI7IHIsT+fcBmBAckKqP5rQVkQkmOqa/NL8Aa2BcMmvrqXGBqfqIvfUxiEiIqlR1wT2T2Ccmb2J10/kbODupEUlIiKSRHUd4eV5M5sM/AzvSoEznHOzkxpZEukidxGRYKtz1aWf7Oqc8MzsaeAkYI1zrl+c5w34N95M8UXAxc65qXV9/12hak8RkWCra5vfzngWOL6G50cAvf3b5cCjSYwlSvgid3V4EREJpKQlP+fcGGBDDaucCjzvPOOBlv60SUmnkp+ISLAls+RXm87A8ojH+f6yaszscjObbGaT165dWy/BiYjIT1cqk1+8Xidxy2LOuSeccwOdcwNzc3d9SNFQhxcV/EREgimVyS8f6BrxuAuwsj42rMlsRUSCLZXJ733gIvMMAQqdc6vqMwClPhGRYEraKC1m9gowHGhrZvnAbUAmgHPuMWAk3mUOC/Audaj3gbJV8BMRCaakJT/n3Hm1PO+Aq5K1/ZpoYGsRkWBLZbVnyoRTn0p+IiKBFMzkF57MVtlPRCSIgpn8/L9q8xMRCaZAJj8REQm2QCa/UIcXFfxERIIpoMnP+6uL3EVEgimYyc//q9QnIhJMgUx+IiISbMFMfqE2PxX9REQCKZDJT5PZiogEWzCTnxr9REQCLZjJT/P5iYgEWiCTn4iIBFsgk1/VdX6pjUNERFIjmMnP/6sOLyIiwRTM5KeSn4hIoAUy+YmISLAFMvmpt6eISLAFMvmhga1FRAItkMlPk9mKiARbMJNfeIgXEREJokAmPxERCbZAJj9Ve4qIBFswk1+ow4v6e4qIBFKwk59yn4hIIAUy+YmISLAFMvnpIncRkWALZvLTRe4iIoEWyOQXotQnIhJMgUx+ushdRCTYApn8QlTrKSISTIFMflXlPmU/EZEgCmTyS/OrPUvLlfxERIIokMlv/84tMINPZq1OdSgiIpICgUx+3do0pn+XlkxcvD7VoYiISAoEMvkB7NOxOXNWbdG1fiIiAZTU5Gdmx5vZPDNbYGY3x3n+YjNba2bT/duvkxlPpL4dmlFYXMaaLSX1tUkREWkgMpL1xmaWDjwCHAvkA5PM7H3n3OyYVV9zzl2drDgS6do6B4AVm4pp3zy7vjcvIiIplMyS3yBggXNukXOuFHgVODWJ29shHVt4ye+M/45jzI9rUxyNiIjUp2Qmv87A8ojH+f6yWL8ws+/N7E0z65rEeKJ0apkTvn/ps5Pqa7MiItIAJDP5xRtDLLZ3yQdAnnPuAOBz4Lm4b2R2uZlNNrPJa9funlJa8+yqGt/ySnV6EREJkmQmv3wgsiTXBVgZuYJzbr1zLtTj5H/AwfHeyDn3hHNuoHNuYG5u7m4Jzsxo16xR+HFJecVueV8REWn4kpn8JgG9zayHmWUB5wLvR65gZh0jHp4CzEliPNW8e9VQTu7fCYBJizfW56ZFRCSFkpb8nHPlwNXAKLyk9rpzbpaZ3Wlmp/irXWNms8xsBnANcHGy4omnU8sc7jxlPwCeHruY8orK+ty8iIikSNIudQBwzo0ERsYsuzXi/i3ALcmMoTYtcjIBGD13DY+PWcRVR/VKZTgiIlIPAjvCS0haWlW/nPyNRSmMRERE6kvgk1+k7Mz0VIcgIiL1QMkvQlaG/h0iIkGgo32ExWu3sX6rxvoUEfmpU/IDXvr1YAA+nV3AGY+OS3E0IiKSbEp+wNBebcP3l65XpxcRkZ86JT9f46yqzi7vz1hZw5oiIrKnU/LzjbzmCI7Zpx0A17wyLcXRiIhIMin5+fLaNmFEv6rR1jTDu4jIT5eSX4QzDurMb4f1BCB/Y3GKoxERkWRR8otgZhzfrwMAs1ZuTnE0IiKSLEp+Mfp2aE6awdgF61IdiogEyBNjFnLjmzNSHUZgKPnFyMlK56yDu/LC+KX85vnJrNNF7yJSD/42ci6vT85PdRiBoeQXxx2netMcfTa7gD+/MzPF0UjQbSoqJe/mj3hrig6MIruLkl8ckQNcFxaXpTASEVjiD7zw3HdLUhqHyE+Jkl8CnVvmAJD5ExnsetyCdRx456ds2V5zMi8sLosa37Sy0nHnB7NZsGZLtXWdc7w1JZ+i0vLdHu/O2LDNKyF9/MOquM9/MaeAUbNW13NUu66i0ptkOc2sljX3bGs2b//Jn2yWVVRSXFqxQ69xzunSqyT4aRzZk+CtKw+jTZMsFq7ZSkXl7tvxKitTsyPf/+k8NhWVMXd19SQW6fB7R3PwXz8PP166oYinxy7myhenVlt3wuIN3PDGDP760ZzdHu+bU/I54d/fMHbBOt6bvqJOr1m8bisAj369MO7zlz03md++MKXW99myvYzlG1I3zN19o+Zy5wezw4/LK7z9JSOtfpLf+q0lPDt28U7tp8453p+xku1ldTvAF2zezqyVhQAM+tsXHPH30Tu8zfemr2D5hiI2FZXyf69Ma9Dt9Jc9N5l9bv1kh15z7hPj6XHLSPJu/oiFa7cmKbL4lqzbxvyCmo8ZeyolvwQ6tMjmzlP7sWJTMXd9OLv2F9RRzz+O5PI6HIB3N/NLDbHHs/+NWcTIiJLSlpLoUlxpuVfqqHSOhWu3Rh0QN24rBWDtlt1/sPnDGzOYvWozFzw5gd+/Oj3uOkWl5Tz4+Y/hGCu8P2yL+AwzVxRSWLRjpYnTHhnLEf/4cucCr4Vzjjs+mMXMFYUJ13nky4U8PXZx+HGp/8HS6in53fTW99z+wew6X+5TUl5BmR/jd4vWc80r07j347k1vuaVicuYsXwTR973JSc+9G14+ebtO1aLUFpeye9fnc5Zj33HvR/P5YMZK/l45urwc2c/9h3jFqa+53ZRaTmTlmxgzI9ra123PLQj+yYs3hC+/8nM5NdcfDl3Tfjkb/j9X3Hsv8bs3PvMW8Npj4zdrYWH3UnJrwYn7N+Bw/Zqw7PjlnDD6zP4wxszGD23APB+8HU9uw0J7QSfzS7YpbhWF27nqpensrpw+w6/NvZs/u6Rc/jdS9VLdSFb/USycO02jv7n1zw8ekH4udBBOXYexB8LtvD57AK2lpRz/6h54eS0u/378/k8+Pn8cMlws19lVuRXKznnOOk/33LW4+O4b1TNB+NIC9duq/O6znnVwqHSS23WbCnhmbFLuPz5yVHLC4vKEv6fQp+nriW/wuKyuNXUdbV2q3dSU9f9u8+fP+Hsx78Dqr6DFZtqHiTilrd/4NRHxrK9bNf2jY1FXqyrN2/nI/8krkVOJgBL129j4pINjF+0gf98MT/qpGhHbCoqpayikoVrt5J380dMWrIh4brjFqxjybrq+88Nr8/grMe+Cz+urHRMWrKB7/M3VVu3tCLx/yQ2kUxZupEJi9bX5WPUiXOOS56dxMkPf1v7yngnGHNXxz9JuvHN75m+fBOrCqv2hSPv+5Jb3v5ht8S6q5T8amBmDOjWEoC3pubz5pR8Ln12MiP+/Q3D7/uK/W8fxcK1W7nl7e/rdIBfWcsBIZHKShc+EH38wyqG3PMFH33v/f1iTuJE+vnsAh78/Efvs/jLyvwqtMpKV2PvwdCZfGyVxz8/+5GSci+WEv/AlZUevRsd968x/Pr5yTw8egEPf7nAr3ItrbaNxeu28cCn81hVWMz05dUPArUp2FyV/DdvLwu3F4USdihp/FiwlUe+rF4VOnNFYY0H+Ngz8PtHzePql70ThVGzVpN380fMK9jC02MXc+4T4+sUcygpxJbi+t/5KVe8OCVuVWOoTTU9zSivqKwWV0hpuffcmY+O45gHEp+tby0pD/+v4m0v3Q/tP6MXRCXp6cs3ccrD38ZtN562rG7f37iF6/h2fvWS2M6eIK3fWrVfbfFLjaHvNDRDy0NfzOefn/3IfreN4rJnJ/H21Np7zX4zfy1f/7iW0/87lgPv/IzrXpvON36p7f3piQe+P//JCQy//yvA++3k3fwRFz8zkXELoxPU9vIKLnxqAqc8PJZ1W0uiEmpJxAlB7Hdd7ie/0vJKrnppKr94dBznxNn3Ppm5mns+nrNDJ+gL1mzhF/6UbptqqC1xzjF6bgFrtmzn3o/ncvyD38RtJmjfvBEAyzdUHfeWri/ilYnL6hxTMin51aJNk0bVls1ZtZlVhdspq3A88NmPvDJxeTgJrdtakrBEFu+MsaLS8fbUfJatLwoflEb8+5twVc281Vu45tVp9P3LJyxZt40rY0pplz03OXywj/Xr5yfz4Ofzow4soQPpW1PzueGNxBfUzlu9hfemr+DmOGdpoeqwTcXegSfTP1qWV1QyZ1XVWWBoW0+MWcSpj4ytHt9zk3ho9AIOvWc0p8V5PtKT3yzik5nRHVlCn/u/Xy3kgNs/DQ9JV+kfIDYl6DxRWOwlypP+8y2/eHQcG7ZVHUAjS+VFEQeOLdvLePjLBXz4/SoWrt3KU9961ZKzVmz2n4/+DioTVPWEToCaNsqotu7ouWvYXFz9uwwl8fQ04/C/f8nJD3v/q6nLNvLkN4vCHSL2/vPHXPT0ROav8dqFYg+cs1du5uUJyxjyty/of8ennP34d1z9cvVB3NP9xPz1j2v5dHZBuLTxz0/n8X1+IWN+9PbNt6fm878xi+J+zkTO/98EfvnUhGrL10a008UroY1bsI5zn/gu/JkWrNnCwrVbWb+tepX7BzNW4pxjaZwD8hdz13D969H7/ZJ127jxzRlRv5MLn5rIr56eGE7qH36/itBXGvr//FiwpcYOZN/4Sf6reWurdeS5/f1Z4VJv/sbiqFJhSUQcV7wY3UQS+vxL128Ll3Tj+fO7M3n860VMWrKBwuIyJtdQWq3a1lSm1uEkZu7qLVz67GSG3juaqcs2ArAq4phXWFzGa5OWsXSd9/9ftmFbVOwhV708lYF//azW7SVLRu2rBFuHFtk1Pj/P70AyeelGZq4sDJcwltx7IlDVAeDIvXN5cfxSAJo1ymDFpmI6t8zhm/lruf71GezbsTmzV22maaMMtpaUM23ZJgb3aMPPH6w6g7/m1fizTWzcVhp1MIWqkht4B60K/wz/8hem8ONfR7Ampp1u7urN9GjbJPz4pP8krvYItfVtjDk7/Psnc/nfN1VtVV/MWRO+v3R9EXd9OJszD+5Cn/bNmLx0Y7Veb1u2l9EsOzPuNkOdakL/V6hKfov9aqZQh5dQ+2a80ibAofd8wQuXDQK8RH7QXZ+F3/dvI6s67xSVVNDcjyeyLWr0nDXh9448gbjxzRmcc0hXxi/awENfzGfqX46luKyCW97+gVtP2peurRuzwk/QTSK+r8gkOydOFVJxRLXn6s3bWb15Ow+Pns/9n3ql+o9nrg5X/0WWMLZsL6dVkyzASygnPPRN1PtO9NuS7tleRvPsTO74YBb5G4vD/7+QzcVltGqSRW5T70Rw5spCTjygY7UksrpwO9tKvFg/m13AO9PyOX1AF8ArqZTVUJ13V0QHn4LN2+mZ2zTq+etfnxH+7F1aNQ6XbP9y0r7V3uub+esYcs8XCbcV65wnvqNgcwkXHZpHv84tEia0Sv83ZOZ9nuP+NYYjerfl8QsPJiczPVwqA5iydEON1ZeRF7OH9tuQu0fOYUjP1lwwuDufR/yGoKrkVxJTUi6rqCQzogYm1OlnVeF2HvjMS+I//nVEtSaKSCtixjOOLDWOnlvALW//wDu/Gxou5ZVVuHCNzed+T+qj+7bj/CejT25ueusHcrIyGLpXm6jlH32fOHnXByW/Wozo14E3rjiUDdtK4/YUXOCfZYdKArGmLtvI71+dzjkDu7LOr6LZUlLO0HtHM/3WY7n4mUne+/i9uEIH9OfGLeG+UfOi3uv7/PjtShu2ldK1dePw44pKR+8/fRx+/HjM2fnDo+eTEVNVefyD39C7XfQBJ5FQ0gslwaLSCl6asDQq8UH1dp+nvl3MF3MKuPDQvLidiB76Yj5/OrH6wSyke5vGUY8LY0pJoe0Vl1XgnEvYbb6otCL8vcXq2bZJOJmGSq4zlm+Kahf9dPbqaokfvANa5EFt6foiHvpiPp/NLuCI3m256NC8cClzytKN3PreTO48tV9USSdeSSz0/ZVWVB1cQ4kv9F7xvDNtBZce3gOA38S0MUZaXbid5tmZPDN2Sdzn7x45hyuO3Iu3p3ltqys2FsetLo1NODe99QOnD+jCzBWFNZ5MAXwScQnK8o3F4eQXSvzZmd7+esZ/xzHhj0eH103UGa1gc82dsF74bgkXHpoXte4y/6B+2XOT4r4m1GSQbsZrk5cDXqLd99ZR3HXqfpzcv1N43V88+h3XHtO7xhhCrnst+iTigxkr+WDGSi4Y3L3aukWl5VRUumql43VbS8jfWMy701ZE7fcFhdvDpddNRaW0a574ZD62TfmHiE5ZL45fRsHmEv7+yVz6dGhW7bVP+PtovCptgGfHLuYl/+S/oVDyq4WZcUheawC+uOFIjv7n13V63bvTVvDxzFWMmuVVo4V+LJHemVbVhT+2zSO2ZFaTDf5Zf3FpBR/PXBVVeonnodEL6BtnB56fICHEentqPg+Pnk9Olrf7FJVW8Kc6joSzxC8BxvO/bxazZH1Rwi72S9cXMXHxBgb1aI1zjvyN0dVac1d5pfCKSsdHP6wivYbr4mLbYB7/eiEtcjKjuskXlVbwY8EWvl2wLpxYD+/Vlm/rOO7ryk3FzFrlHUBCHynywPT8d0tp3zw7qrds5PY//mEV/Tq3CD9fl56uZGb7AAAbO0lEQVSCke78cHY4+cV+3kgbtpVWq1KO9OaUfCYsrnp9webtdZrwubS8krEL1nHBk9WrOWuybEMRJeUV/OXdmbw+OZ/m2Rl08q+7XbOlhNWbd7yjV6y/vDeLHm2bcnjvtuFlNXX8Aq9mA+DJOCe6705fyfA+7aKW7WpnnnheHL+M1YUlnD+4a9TyNZtLoqpOQ/75WdVJ0udz1nDOIV1JTzO2lpQzfuF69unUnCXrtnFIXutqPb3/FfHa0XO9Euh7Ee2dQ3u1YeyC6P1qXoLLImKrUzfXcr1xfbA97eLJgQMHusmTE5/FJtv2sgp+/dzkOh8AY2VnpoV/FLnNGu30ZQLNGmWEd9a/ntaPb+avZVNRWVS36ETSDOrS+/jCId355ZDuUVWvdX1tMs264+ds3l7GoffUfE3YPWfsn7BnWWa60a1142o9O1vkZNLUr5YOadu0UTgp/fvcAxNeehHrokO78/KEZZRXOq4/dm+uObo3V744JdwVP5HubRqHO2sM6dma8Ytq/04T+fz6YfRq14y8mz/a6fcIadk4k26tG7N43TYy0ixu6Xd36Nwyh+1lFayPaIsNNQdEOrl/Jz7wk/CIfh1YuamYGQlqRy47vEfC2pndJSsjLeok9qyDu/DGLgxJ98j5B3HVy/ET8n/OG8D/RUy6/cSFB9fpEqq7TuvH+EXrq1U5jvl/RzHsvi+5cEh3Xhi/lIw0I69tk4Q1JABnDOgcrg3YFQvuHlGtJmpnmNkU59zAuq6vDi87KDsznfvP6k/LxvHbpnIihkY7yO8pGmlwj6p671Diu+u0flHr1GUgj8N6Vb3PBzNWMmpWQdzEF6ouitS+hqqPSMP75Far4qhL4rtlRN9wL9l42jbNqrYstkqzJsf9a0z4oBerdZOq9w5VMd535gHV1iurcFx8WF615YXFZewVU/0bWRrLa9Mk9iUJPf/d0nAbTWFxGeUVlWwsKqVD82zOGNA54etOjag+G79oA4f2bMMgv/bht8N68vn1w5j4p6N568rDao3hmAfG8MJ3S+occ02O27c9A7u3Zsv28mqJb0cGn7l0aA9OPbATI685gj+dsA+tGmfSyv89hU48IhMfELdTV2TtxX8vOIh3rxqacJsXDO7G3u3rVq2/s2Jrb3Yl8QEJEx9U7xRUEFMazm3WiGF751Z73WNfLYzb1vbUt1615VF9c/nziftQXulYsGYr+3RsHl7nrSsPDd9/8JwDyW1WvTMgeCcl/zyrPxB9GdTFh+WxV27138/2JF0KVRslv53QoUU2Vx65V/hxv85VO8iD5x4Yvv//ft6XMwZ05pmLD+HYfdsDcMw+7Zj2l2P57Lph4fViqyB7xTT2P/Wr6iczOZnpPPbLg4Do6sqDu7eKWu+ALtWT0Kparg8MHST28uMIJfrbTq7eHnd033bVlv32yL3iJpaQDi2yuf7YvaOWRXa2qc2KTcW8P2MlfTs0o2fMj2mfjlX/y3enraBpowx+cVAXbj1pXx4+fwBvXlH1Ax7QrRV3nLJftfffr1PzastCOrfKqVOMoYN5mkHHFtkUFpex322jGL9oA/t2as4x/v4Q6ZHzD+KZiw+J2kbrJlm8cvmQcEm0T4dm9GrXjHbNssOdnBpFHGDiJaG/vDerTjED/PGEvok/U5MseidIIIN7tK7zNi4Zmse/zx3Avp2a85thPZl263GMu/lobj1pX+4+vV/tbxBnm2YW1VGncVZ61LpdWjVmYF7V+hcflscPtx9H/67Rv4+BEb+fUC/mkEF5df+MO+rG4/vw7CWH1Hn9z2MucXppQvTlA307NKNZtrd/XDI0L7w8tB/F/t6e+85rj+vUMocurapORI/dp+r3PaBr1f+mf9eWtPU7QLVv3iiqvbBn2ya0auLt/9kR++afT9yHvh2q/7biXRdZH5T8dlJo57n3jP1584qqM/BhvXOZf/cIPr9+GIfu1YYHzjmQo/q2o7vfIWVbaYV/EGnG59cfyftXD4062D57ySHceHxfItue+3VuUW37OVkZHN+vI0f0bhvVVX//zi2Y8MejOW9QN8DrFfa30/cPP/+PXxzA8D7eGeHUvxzLtL8cW+29R15zBHPvOp48/zN+dt2RfHrdMC4Z2oNPrj0iat27TusX96DQqnFVCSz0Iwnp0DyHq47qFbWseUwvz6z0NM4f3I2rY9YLmbliM93bNKZNk+hS5GF7VbXhzF+zla0l5aSlGZce3oOTDujEwLzW3Hh8Hw7bqw292zetdpCE6icjNcWZyHH7epMiZ6Sn0SInkw3bSsM99FrkZDI0Is6QPh2acVTfduGOFQBd/EQYOmgN6FZ1AOrRtgnD++Ty8m+GhJdF1jzU5KBuLav9b888uAu/OaIn3950VDiJfnTN4eT5pfI2TbI4e2BXrvBP/EInDj/r247HfzmQFy4bRLeIjldDesZPFvFKDDlZ6Vx6eI+4JetQ6aN/lxaMv6Wqs8tB3VpVWzckNDbveYO6ckCXFmRlpHHbyfuGlx/Vtx3NsjM5vFd0D8Sf79eBCX88msX3nMDsO48P/xa+ufEoXvrNYM4ZWNXWduL+HRNuf0d1bdWY7hGfPbKU2imix/kZB3k1BrG9QGOHLbzz1H7hS2giS28Avdo15cs/DA9/r5G6t25C19ZVJ19Delb9fyKvTW3bNCv8PZaUV7LgbyeEn+uZ24SW/u9/e1klN4/oy/A+uWSkp0W1eYdqpc7/X92ukd3d1OFlJx23XwdGXTssXC0YagvLzkzDzOjVLvoAOrxPO578djEHdKlKZL0iqtf6d2nB4J5two3m8+8+gb3+OBKI7hZ/8WF5PDtuSfggd0he6/D1ROCV0to3z+aXQ7rxysRllJRVcP7gbpy4f0eyMtLIyUrn5P6dKCotj6oiDOnWujEZ6WlkRBxDc5s1Cu/ofTs05/lLvcsENhaV0qllDs1zqu9GkVWr3950FH3/8gnd2zTmlP6duOjQPNLTjM+uG8b3+YXc8MaMqLbPO07Zj8N7t2Wv3Ka8W0ObQqeWObTMyWLSko3cdVo/Gmemc+IBHfl01uqEbT8Avxvei98N7xX1v+3boVn4APKzvu146LwBHN23Ha9OWs5dH84mM924/ZT9auwqPiivNVf9rBfzC7awb6fmvDZ5OWkGzXMywx0GvG2m06JxJr8d1jOqJ26oBBd5CUiohNytdWOWbSiKOmBlZaTx7CWDomIINeFf87NePBQxGk/nljnhBDrlz8fQxj8h6d2+abgNs7i0AjOjS6vGZKQZZRWOJlkZ4VlOWjbOIj3NuHlEX24e4ZUQTxvQmTSDZtmZHNE7l5G/P4KCzdtp3TiLy1/w2uZ/c0SPqJ7A2TUk6MheyyFnD+zCHR/MZvnGYjq0yObLPwxnm39Sk5OZzlkDu1R7zX/OH8CrE5dz60n7hg/ajTLSeed3h/HVvLUM8zu6XHvM3vRq15Tv8wt5ZuwSju/XIbzvZqYb+/onpqG4/n7mAcxcWcislZu5YHC3atfaDe7ROmG7++G92jJ9+SbuOWP/cHtdxxbZrCrcTpdWOXSMSHKRI7m8d/XhHHK3N97uA2cfyB+O68Nh91Zv7+7ftSUzlm8iJzOdHm2bhPfV5tkZLPrbCZz52DimLtsUrlbv3CqHJeuL+O2RPXn8a28/zMlKjzqB6ZHbhH+ceQDN/VJkqF2zaaOM8P8k9oL4Ds2zwye/pRWVXHHkXuETpsgTn2G9c/l0dkG1y7Tqi5LfLohsD/v0uiOZtbKw2jVSIYf3bsuM244LD70U672rD496nJ5mPH3xQLLS02mcmY4Z/PnEfTmlfyemLd/EZUd4vfgGxlRzhkomoaqLcw/xzlRbRLRR5mSlkxNR4nnt8iGUVzoueHJCjQf3kNi2hHhn8nltq35A2ZnpvHDZIPp08KrrQnq3bxa+NqlJo6p4LhzSPXzAijdCxaAerZm4eAMtc7K49pg8+nVuzgWDuoVf897Vh7OqsLjWDjFQdcFyxxbZ4eTXLDuTU/wDRKhaNc0s3PU8MlECPHnRQH79/GQqnePIvXM5cu9cfvR7vaWZcUK/DuFr6o7dt324SvjIPrk8PmYRz1x8CFtKysMHk9BF7Vcf1St8MvT27w5jc3FZwv0rVmTV6fhbjqZZdga/enoilw/rGU58AKce2JlWjbO46OmJUReaezNIOBpnpTOkZ5uEA6LH7s9NG2XQ1K8u79qqMZOWbOSI3rnVLoNJpFXjTM4e2CXqkpGj+7anuKwiXNKLrLKbc9fxUa8f8/+8UmvX1o25PU6Vdrvm2Zx9SFXpLTM9jdMHdOGE/Tty/qBucZNvrHi1BVkZabzzu8PYr1ML/vvVAv7xyTyG9mrDtcfszcI1W7n57R+476wD6NjC+15yMtMpq6hk3ML1vDB+KV1aNY46KQjlvlMP7FTt99WpZQ6De7Rm0pINfHLtMLZsL+NvI+dyxyn7cdJ/vg3XGt0yYh8aZ2UwvE870tKM3x+zN1OWbuS3fiJa41/i0btdM84b1C18aU+z7ExyMtMpLqugfbNszo4o7X5x/ZEsWLsVM6NngqaK3GaNwtX+se44dT+aZ2ewbmspJ/fvxKezC2is5Ldn69WuaVRJLp5EiS+Rn/WtahdafE/Vxd3vRTTsD+jWigO6tKBg83YKNpdQXllVtbb4nhPqdLAc3LMNlZWOS4bmccHgbjsUI8CNP+9Leprx4viqdodGGdEHiCN6V298B8hr24SHzhvAYXu1YaA/m0Rk9UqoTcbMK9XcdHxfThvQiWtfnc7R+7SjRU5m+HqtSB1b5DCkZ2uO7lu9bS1SqONA6OL6RG1AkRcV/++igfxYsIXLnvNKNqETi4qIntOhM980M84b3I3b/Yu47z+rf3g/OGyvtsy96/hqJaHzBndlzPy1XHRo1XVebZs2qlZ9HOmLG44kKz2NY//lXYoTujQAqgZqeDNBB5lQtftpB1Z1wslKT6OkvJJGmen8v5/3oUmjdE46YMeq+e48rR/D+7Zj2N65jL/laNZvK6m1+7+Z8Y8z+0clvy6tcsIl9dp024GOU5EaZaTTu33i6u5I1x/bh8ufnxwuFQLMu+v48G8tsmYBvNqZcw7pGvVbDLX5Hr1Pe351WF61BHfB4G789aM53HmK1wZ6zD7tiewQ+fJvhlBaXhk+iX3rysPC192GTmA7tMjmnjOqmjxCJ2YhlwztwR/f+YER/Tpw5sHRpedvbzqK/I3F1Ybh69q6cfgEoVWcmiOAts0a0TQrfmppnp3JHad6n2ms32O+SZyTiXoRGhppT7kdfPDBTqp7/OsFrvtNH7rHv16QshjOfHSse+LrheHH70zNd+9NX1Hn1y9bv80tXbet2vLyikp3+fOTXPebPnRfzVuzW2INWbWp2PX+40g3Y/lGV1hc6opLy6ut88Cn89yomauqLd/7TyPd716c4tZvLXHdb/rQvTstPyrmMx8d60bPKXDOOXfD69Pdbe/N3K2xx/P6pGWu/x2j3LaSMjfsH6Pd/708tU6vK6+ojHr8Q/4md9t7M11lZWWCVyTX98s3ucF3f+5++eT4lGy/ru78YJYbcOenu+39Xp+0zL08YamrrKx0peUVO/TayspKd9t7M90P+Zt26DW7YtTMVeHtdb/pQ9f9pg/D73nmo2PdS+OXJnztmB/XuO43fejOe+K7XYohBJjsdiCX6Dq/n4iS8goe/3oRlw/rWWObyp5qzebtPPntYm78eZ/dck2QiOxeh93zBSsLt0cNQViT0XMLuPTZyZzcvxP/OW/ALm9/R6/zU/ITEZFdVlhURlFZebhdszblFZXc9+k8fjtsr7id73bUjiY/tfmJiMgua9E4kxbUvV9DRnoat4zYJ4kR1Uz1RyIiEjhKfiIiEjhJTX5mdryZzTOzBWZ2c5znG5nZa/7zE8wsL5nxiIiIQBKTn5mlA48AI4B9gfPMLHZwyMuAjc65XsC/gL8nKx4REZGQZJb8BgELnHOLnHOlwKvAqTHrnAo8599/Ezja6jqEhYiIyE5KZvLrDETO4JrvL4u7jnOuHCgE2sSsg5ldbmaTzWzy2rU7NqGniIhIrGQmv3gluNiLCuuyDs65J5xzA51zA3Nz4w+TJSIiUlfJTH75QNeIx12A2BlIw+uYWQbQAtj5aatFRETqIJkXuU8CeptZD2AFcC5wfsw67wO/Ar4DzgRGu1qGnJkyZco6M1u6G+JrC6yrda2GR3HXrz01bthzY1fc9eunEnf3RCvGk7Tk55wrN7OrgVFAOvC0c26Wmd2JNwDp+8BTwAtmtgCvxHduHd53t9R7mtnkHRkKp6FQ3PVrT40b9tzYFXf9CmrcSR3ezDk3EhgZs+zWiPvbgbOSGYOIiEgsjfAiIiKBE+Tk90SqA9hJirt+7alxw54bu+KuX4GMe4+b0khERGRXBbnkJyIiAaXkJyIigRO45FfbTBOpZmZPm9kaM5sZsay1mX1mZvP9v6385WZmD/mf5XszOyiFcXc1sy/NbI6ZzTKz3+8JsZtZtplNNLMZftx3+Mt7+DONzPdnHsnylzeomUjMLN3MppnZh3tK3Ga2xMx+MLPpZjbZX9ag9xM/lpZm9qaZzfX380P3kLj7+P/r0G2zmV27h8R+nf+7nGlmr/i/192zjzvnAnPDu95wIdATyAJmAPumOq6YGIcBBwEzI5b9A7jZv38z8Hf//gnAx3jDxA0BJqQw7o7AQf79ZsCPeLN5NOjY/e039e9nAhP8eF4HzvWXPwZc6d//HfCYf/9c4LUU7y/XAy8DH/qPG3zcwBKgbcyyBr2f+LE8B/zav58FtNwT4o75DOnAarwLwht07HhjPy8GcvzHrwMX7659POVfRj3/Mw8FRkU8vgW4JdVxxYkzj+jkNw/o6N/vCMzz7z8OnBdvvVTfgPeAY/ek2IHGwFRgMN7IERmx+w3eoA2H+vcz/PUsRfF2Ab4AfgZ86B+s9oS4l1A9+TXo/QRo7h+ILWZ5g447zuc4Dhi7J8RO1cQHrf199kPg57trHw9atWddZppoiNo751YB+H/b+csb5OfxqxsG4JWiGnzsftXhdGAN8Ble7cAm5800EhtbnWYiqScPAjcClf7jNuwZcTvgUzObYmaX+8sa+n7SE1gLPONXMz9pZk1o+HHHOhd4xb/foGN3zq0A7geWAavw9tkp7KZ9PGjJr06zSOxBGtznMbOmwFvAtc65zTWtGmdZSmJ3zlU45w7EK0kNAvaJt5r/t0HEbWYnAWucc1MiF8dZtUHF7RvqnDsIb6Lrq8xsWA3rNpS4M/CaIx51zg0AtuFVFSbSUOIO89vGTgHeqG3VOMtSsY+3wpvztQfQCWiCt8/E2ql9PGjJry4zTTREBWbWEcD/u8Zf3qA+j5ll4iW+l5xzb/uL94jYAZxzm4Cv8No5Wpo30whEx9ZQZiIZCpxiZkvwJor+GV5JsKHHjXNupf93DfAO3glHQ99P8oF859wE//GbeMmwoccdaQQw1TlX4D9u6LEfAyx2zq11zpUBbwOHsZv28aAlv/BME/5Z0Ll4M0s0dKHZL/D/vhex/CK/d9YQoDBUjVHfzMzwBiqf45x7IOKpBh27meWaWUv/fg7eD24O8CXeTCNQPe7Q56nTTCTJ4Jy7xTnXxTmXh7cfj3bOXUADj9vMmphZs9B9vDaomTTw/cQ5txpYbmZ9/EVHA7Np4HHHOI+qKk9o+LEvA4aYWWP/+BL6n++efTzVDbApaEQ9Aa8n4kLgT6mOJ058r+DVb5fhnclchldv/QUw3//b2l/XgEf8z/IDMDCFcR+OV8XwPTDdv53Q0GMHDgCm+XHPBG71l/cEJgIL8KqJGvnLs/3HC/znezaAfWY4Vb09G3Tcfnwz/Nus0G+woe8nfiwHApP9feVdoNWeELcfT2NgPdAiYlmDjx24A5jr/zZfABrtrn1cw5uJiEjgBK3aU0RERMlPRESCR8lPREQCR8lPREQCR8lPREQCR8lPAs3Mxvl/88zs/N383n+Mt61kMbPTzOzWJL333Wa23My2xixPOJK+md3iL59nZj/3l2WZ2ZiIi5RFUkLJTwLNOXeYfzcP2KHkZ2bptawSlfwitpUsNwL/3dU3SfC5PsAbiSXWZcBG51wv4F/A3/332Bfv4vv9gOOB/5pZunOuFO+asnN2NU6RXaHkJ4EWUZK5FzjCvPnOrvMHu77PzCb5c5r91l9/uHnzFr6MdwEwZvauP0jzrNBAzWZ2L5Djv99LkdvyR864z7w5yn4ws3Mi3vsrq5oz7iV/ZAvM7F4zm+3Hcn+cz7E3UOKcW+c/ftbMHjOzb8zsR/PGAg0N4l2nzxXJOTfexR/l41S8qX7AG/LraD/mU4FXnXMlzrnFeBceh5Lnu8AFdfuGRJJDVQ8inpuBPzjnQknicrxhnQ4xs0bAWDP71F93ENDPP6gDXOqc2+APjzbJzN5yzt1sZlc7b8DsWGfgjRbSH2jrv2aM/9wAvNLSSmAsMNTMZgOnA32dcy40HFuMoXjTMUXKA44E9gK+NLNewEU78LnqImokfTMLjaTfGRgfsV7k6PszgUN2YBsiu52Sn0h8xwEHmFloDMEWQG+gFJgYkyCuMbPT/ftd/fXW1/DehwOvOOcq8AYX/hovGWz23zsfwLxplvLwksh24Ekz+whvXrNYHfGm3In0unOuEphvZouAvjv4ueoi0Uj6CUfYd85VmFmpmTVzzm3Zwe2J7BZKfiLxGfB/zrlRUQvNhuNNZxP5+Bi8STSLzOwrvDEGa3vvREoi7lfgTdpZbmaD8Ab2PRe4Gm8Wh0jFeIksUuzYhaGkVOvn2gGhkfTzLXok/dpmBmiEl9BFUkJtfiKeLUCziMejgCvNm6YJM9vbvFkIYrXA6/BRZGZ98aZDCikLvT7GGOAcv/0tFxiGNxBvXObNkdjCOTcSuBavyjTWHKBXzLKzzCzNzPbCGwx43g58rrpKNJL++8C5fm/QHnily4n+NtsAoWlqRFJCJT8Rz/dAuZnNAJ4F/o1X5TjV78CxFjgtzus+Aa4ws+/xkktkO9cTwPdmNtV50w2FvAMcijezgQNudM6t9pNnPM2A98wsG6/kdl2cdcYA/zQzc1Wj1c8DvgbaA1c457ab2ZN1/FxRzOwfeL1hG5tZPvCkc+52vGmsXjCzBXglvnMBnHOzzOx1vCloyoGr/GpegKOAkbVtUySZNKuDyE+Emf0b+MA597mZPYs3zdGbKQ6rGjN7G7jFOTcv1bFIcKnaU+Sn429487Y1WOZNIv2uEp+kmkp+IiISOCr5iYhI4Cj5iYhI4Cj5iYhI4Cj5iYhI4Cj5iYhI4Px/6lThVv0lqosAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'parameters_v5' (dict)\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "# train 3-layer model\n",
    "layers_dims = [shuffled_X.shape[0], 530, 200, 1]\n",
    "parameters_v5 = model(shuffled_X, shuffled_Y, layers_dims)\n",
    "%store parameters_v5\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the training set:\n",
      "Accuracy: 0.938440642170588\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "#print (\"On the training set:\")\n",
    "#predictions_training = predict(shuffled_X, shuffled_Y, parameters)\n",
    "#print (\"On the test set:\")\n",
    "#predictions_testing = predict(shuffled_X_test, shuffled_Y_test, parameters)\n",
    "\n",
    "\n",
    "# train 3-layer simplified model\n",
    "#layers_dims = [shuffled_X.shape[0], 10, 3, 1]\n",
    "#parameters = model(shuffled_X, shuffled_Y, layers_dims)\n",
    "\n",
    "# Predict\n",
    "print (\"On the training set:\")\n",
    "predictions_training = predict(shuffled_X, shuffled_Y, parameters_v5)\n",
    "#print (\"On the test set:\")\n",
    "#predictions_testing = predict(shuffled_X_test, shuffled_Y_test, parameters_v5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the test set:\n",
      "Accuracy: 0.938901179941003\n"
     ]
    }
   ],
   "source": [
    "print (\"On the test set:\")\n",
    "predictions_testing = predict(shuffled_X_test, shuffled_Y_test, parameters_v5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
