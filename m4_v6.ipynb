{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version \"m4_v6\" is with an input of 11 variables where we kept the chi2 of the Kaon and the instentanious intensity Lambda, since we're getting better predictions to lower than 2% on missing signal. We will start by training a (11,530,200,1) FCNN. We plan to increase the number of variables later...maybe add layers even\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/na62/jcarmignani/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# %load m4_v3.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Thu Aug 23 10:46:07 2018\n",
    "\n",
    "@author: jcarmignani\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jun 23 20:42:37 2018\n",
    "\n",
    "@author: J_Carmignani\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "\n",
    "\n",
    "#reg_utils-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "                    \n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    #m = X.shape[1] # number of examples\n",
    "    #layers_dims = [4, 10, 3, 1]\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = (np.random.randn(layer_dims[l], layer_dims[l-1]))*(np.sqrt(2./ (layer_dims[l-1])))\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    #assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n",
    "    #assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 256, seed = 0):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1] # number of training examples\n",
    "    mini_batches = []\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(math.floor(m/mini_batch_size)) # number of mini batchesof size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "# Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "\n",
    "        mini_batch_X = shuffled_X [:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y [:, num_complete_minibatches * mini_batch_size : m]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the loss) presented in Figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape ()\n",
    "                    b1 -- bias vector of shape ()\n",
    "                    W2 -- weight matrix of shape ()\n",
    "                    b2 -- bias vector of shape ()\n",
    "                    W3 -- weight matrix of shape ()\n",
    "                    b3 -- bias vector of shape ()\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the loss function (vanilla logistic loss)\n",
    "    \"\"\"\n",
    "        \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)+1e-8\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache\n",
    "\n",
    "def compute_cost(A3, Y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "    \n",
    "    Arguments:\n",
    "    a3 -- post-activation, output of forward propagation\n",
    "    Y -- \"true\" labels vector, same shape as a3\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the cost function\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    a=np.reshape(A3,(1,m))\n",
    "    #logprobs = (np.multiply(-(np.log(a)),Y) + np.multiply(-(np.log(1 - a)),(1 - Y)))\n",
    "    cost = (np.nansum((np.multiply(-(np.log(a)),Y) + np.multiply(-(np.log(1 - a)),(1 - Y)))))*(1./m)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def backward_propagation(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 =(1./m) * (A3 - Y)\n",
    "    dW3 = np.dot(dZ3, A2.T)\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.float64(A2 > 0))\n",
    "    dW2 = np.dot(dZ2, A1.T)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.float64(A1 > 0))\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def initialize_adam(parameters) :\n",
    "   \"\"\"\n",
    "   Initializes v and s as two python dictionaries with:\n",
    "   - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n",
    "   - values: numpy arrays of zeros of the same shape as the corresponding\n",
    "   gradients/parameters.\n",
    "   Arguments:\n",
    "   parameters -- python dictionary containing your parameters.\n",
    "   parameters[\"W\" + str(l)] = Wl\n",
    "   parameters[\"b\" + str(l)] = bl\n",
    "   Returns:\n",
    "   v -- python dictionary that will contain the exponentially weighted average of the\n",
    "   gradient.\n",
    "   v[\"dW\" + str(l)] = ...\n",
    "   v[\"db\" + str(l)] = ...\n",
    "   s -- python dictionary that will contain the exponentially weighted average of the\n",
    "   squared gradient.\n",
    "   s[\"dW\" + str(l)] = ...\n",
    "   s[\"db\" + str(l)] = ...\n",
    "   \"\"\"\n",
    "\n",
    "   L = len(parameters) // 2 # number of layers in the neural networks\n",
    "   v = {}\n",
    "   s = {}\n",
    "# Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "   for l in range(L):\n",
    "### START CODE HERE ### (approx. 4 lines)\n",
    "       v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
    "       v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
    "       s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
    "       s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
    "### END CODE HERE ###\n",
    "   return v, s\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 1e-7,\n",
    "beta1 = 0.9, beta2 = 0.99, epsilon = 1e-8):\n",
    "   \"\"\"\n",
    "   Update parameters using Adam\n",
    "   Arguments:\n",
    "   parameters -- python dictionary containing your parameters:\n",
    "   parameters['W' + str(l)] = Wl\n",
    "   parameters['b' + str(l)] = bl\n",
    "   grads -- python dictionary containing your gradients for each parameters:\n",
    "   grads['dW' + str(l)] = dWl\n",
    "   grads['db' + str(l)] = dbl\n",
    "   v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "   s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "   learning_rate -- the learning rate, scalar.\n",
    "   beta1 -- Exponential decay hyperparameter for the first moment estimates\n",
    "   beta2 -- Exponential decay hyperparameter for the second moment estimates\n",
    "   epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "   Returns:\n",
    "   parameters -- python dictionary containing your updated parameters\n",
    "   v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "   s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "   \"\"\"\n",
    "   L = len(parameters) // 2 # number of layers in the neural networks\n",
    "   v_corrected = {} # Initializing first moment estimate, python dictionary\n",
    "   s_corrected = {} # Initializing second moment estimate, python dictionary\n",
    "# Perform Adam update on all parameters\n",
    "   for l in range(L):\n",
    "# Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "      v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*grads[\"dW\" + str(l+1)]\n",
    "      v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*grads[\"db\" + str(l+1)]\n",
    "      v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-beta1**t)\n",
    "      v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-beta1**t)\n",
    "      s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*((grads[\"dW\" + str(l+1)])**2)\n",
    "      s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*((grads[\"db\" + str(l+1)])**2)\n",
    "      s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-beta2**t)\n",
    "      s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-beta2**t)\n",
    "      parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*v_corrected[\"dW\" + str(l+1)]/((s_corrected[\"dW\" + str(l+1)])**0.5 + epsilon)\n",
    "      parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*v_corrected[\"db\" + str(l+1)]/((s_corrected[\"db\" + str(l+1)])**0.5 + epsilon)\n",
    "       \n",
    "   return parameters, v, s\n",
    "\n",
    "   \n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m), dtype = np.int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    A3, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    file1=open(\"misspredictions_m4v5_Kaon.txt\",\"w\")\n",
    "    file2=open(\"misspredictions_m4v5_Pileup.txt\",\"w\")\n",
    "\n",
    "    for i in range(0, A3.shape[1]):\n",
    "        if A3[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    # print/save results\n",
    "    for j in range(0, A3.shape[1]):\n",
    "        if p[0,j] == y[0,j]:\n",
    "            continue\n",
    "        else:\n",
    "            if p[0,j] == 1:\n",
    "                file1.write(str(j)+ \" \" + str(p[0,j])+\"\\n\")\n",
    "            else:\n",
    "                file2.write(str(j)+ \" \" + str(p[0,j])+\"\\n\")\n",
    "    file1.close()\n",
    "    file2.close()\n",
    "    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n",
    "    \n",
    "    return p\n",
    "    \n",
    "\n",
    "\n",
    "def predict_dec(parameters, X):\n",
    "    \"\"\"\n",
    "    Used for plotting decision boundary.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (m, K)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predict using forward propagation and a classification threshold of 0.5\n",
    "    A3, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A3>0.5)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \n",
    "  #plt.scatter(train_X[2, :], train_X[3, :], c=train_Y, s=40, cmap=plt.cm.Spectral);\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#11vars-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "tchod1= np.genfromtxt(\"tCHODS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "tchod1_test= np.genfromtxt(\"tCHODS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "trich1= np.genfromtxt(\"tRICHS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "trich1_test= np.genfromtxt(\"tRICHS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "tgtk1= np.genfromtxt(\"tGTKS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "tgtk1_test= np.genfromtxt(\"tGTKS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "tktag1= np.genfromtxt(\"tKTAGS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "tktag1_test= np.genfromtxt(\"tKTAGS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "cda1= np.genfromtxt(\"CDAS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "cda1_test= np.genfromtxt(\"CDAS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "chi21= np.genfromtxt(\"CHI2S1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "chi21_test= np.genfromtxt(\"CHI2S1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "chi2p1= np.genfromtxt(\"CHI2PS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "chi2p1_test= np.genfromtxt(\"CHI2PS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "lamd1= np.genfromtxt(\"LAMBDAS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "lamd1_test= np.genfromtxt(\"LAMBDAS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "dx1= np.genfromtxt(\"DXS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "dx1_test= np.genfromtxt(\"DXS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "dy1= np.genfromtxt(\"DYS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "dy1_test= np.genfromtxt(\"DYS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "vtrx1= np.genfromtxt(\"VTRXS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "vtrx1_test= np.genfromtxt(\"VTRXS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "vtry1= np.genfromtxt(\"VTRYS1.txt\",skip_footer=(46746),usecols=(1),unpack=True)\n",
    "\n",
    "vtry1_test= np.genfromtxt(\"VTRYS1.txt\",skip_header=(420709),usecols=(1),unpack=True)\n",
    "\n",
    "tchod0= np.genfromtxt(\"tCHODB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "tchod0_test= np.genfromtxt(\"tCHODB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "trich0= np.genfromtxt(\"tRICHB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "trich0_test= np.genfromtxt(\"tRICHB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "tgtk0= np.genfromtxt(\"tGTKB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "tgtk0_test= np.genfromtxt(\"tGTKB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "tktag0= np.genfromtxt(\"tKTAGB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "tktag0_test= np.genfromtxt(\"tKTAGB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "cda0= np.genfromtxt(\"CDAB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "cda0_test= np.genfromtxt(\"CDAB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "chi20= np.genfromtxt(\"CHI2B1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "chi20_test= np.genfromtxt(\"CHI2B1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "chi2p0= np.genfromtxt(\"CHI2PB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "chi2p0_test= np.genfromtxt(\"CHI2PB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "lamd0= np.genfromtxt(\"LAMBDAB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "lamd0_test= np.genfromtxt(\"LAMBDAB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "dx0= np.genfromtxt(\"DXB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "dx0_test= np.genfromtxt(\"DXB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "dy0= np.genfromtxt(\"DYB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "dy0_test= np.genfromtxt(\"DYB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "vtrx0= np.genfromtxt(\"VTRXB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "vtrx0_test= np.genfromtxt(\"VTRXB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "vtry0= np.genfromtxt(\"VTRYB1.txt\",skip_footer=(128354),usecols=(1),unpack=True)\n",
    "vtry0_test= np.genfromtxt(\"VTRYB1.txt\",skip_header=(1155182),usecols=(1),unpack=True)\n",
    "deltatchod1=tchod1-tgtk1\n",
    "deltatchod1_test=tchod1_test-tgtk1_test\n",
    "deltatrich1=trich1-tgtk1\n",
    "deltatrich1_test=trich1_test-tgtk1_test\n",
    "deltatchod0=tchod0-tgtk0\n",
    "deltatchod0_test=tchod0_test-tgtk0_test\n",
    "deltatrich0=trich0-tgtk0\n",
    "deltatrich0_test=trich0_test-tgtk0_test\n",
    "deltatktag0=tktag0-tgtk0\n",
    "deltatktag0_test=tktag0_test-tgtk0_test\n",
    "deltatktag1=tktag1-tgtk1\n",
    "deltatktag1_test=tktag1_test-tgtk1_test\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "deltatrich=np.hstack((deltatrich0,deltatrich1))\n",
    "deltatchod=np.hstack((deltatchod0,deltatchod1))\n",
    "deltatktag=np.hstack((deltatktag0,deltatktag1))\n",
    "deltatrich_test=np.hstack((deltatrich0_test,deltatrich1_test)) \n",
    "deltatchod_test=np.hstack((deltatchod0_test,deltatchod1_test)) \n",
    "deltatktag_test=np.hstack((deltatktag0_test,deltatktag1_test)) \n",
    "cda=np.hstack((cda0,cda1))\n",
    "chi2=np.hstack((chi20,chi21))\n",
    "chi2p=np.hstack((chi2p0,chi2p1))\n",
    "lamd=np.hstack((lamd0,lamd1))\n",
    "dx=np.hstack((dx0,dx1))\n",
    "dy=np.hstack((dy0,dy1))\n",
    "vtrx=np.hstack((vtrx0,vtrx1))\n",
    "vtry=np.hstack((vtry0,vtry1))\n",
    "cda_test=np.hstack((cda0_test,cda1_test))\n",
    "chi2_test=np.hstack((chi20_test,chi21_test))\n",
    "chi2p_test=np.hstack((chi2p0_test,chi2p1_test))\n",
    "lamd_test=np.hstack((lamd0_test,lamd1_test))\n",
    "dx_test=np.hstack((dx0_test,dx1_test))\n",
    "dy_test=np.hstack((dy0_test,dy1_test))\n",
    "vtrx_test=np.hstack((vtrx0_test,vtrx1_test))\n",
    "vtry_test=np.hstack((vtry0_test,vtry1_test))\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#For Background train\n",
    "mask00= (-1< deltatchod0) & (deltatchod0<1)\n",
    "x00a=deltatchod0[mask00]\n",
    "mask10= (-1< deltatrich0) & (deltatrich0<1)\n",
    "x10=deltatrich0[mask10]\n",
    "mask20= (-1< deltatktag0) & (deltatktag0<1)\n",
    "x20a=deltatktag0[mask20]\n",
    "mask30a= (cda0<15)\n",
    "x30a=cda0[mask30a]\n",
    "\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#'' x00a.shape                                                                    ''\n",
    "#'' x10.shape                                                                     ''\n",
    "#'' x20a.shape                                                                    ''\n",
    "#'' x30a.shape#to check the dimentionality then take the lowest dim mask for all  ''\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "x00=deltatchod0[mask10]\n",
    "x20=deltatktag0[mask10]# to fix the cut having the same shape\n",
    "x30=cda0[mask10]\n",
    "mask30=(x30<15)#fix the condition for mask30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x330=x30[mask30]#if lower dim apply it again for all\n",
    "x220=x20[mask30]\n",
    "x110=x10[mask30]\n",
    "x000=x00[mask30]\n",
    "#continue the same for all the other variables to unify the dimensions\n",
    "x40=chi20[mask10]\n",
    "x440=x40[mask30]\n",
    "x50=chi2p0[mask10]\n",
    "x550=x50[mask30]\n",
    "x60=lamd0[mask10]\n",
    "x660=x60[mask30]\n",
    "x70=dx0[mask10]\n",
    "x770=x70[mask30]\n",
    "x80=dy0[mask10]\n",
    "x880=x80[mask30]\n",
    "x90=vtrx0[mask10]\n",
    "x990=x90[mask30]\n",
    "y90=vtry0[mask10]\n",
    "y990=y90[mask30]\n",
    "\n",
    "nx000=(x000)#-min(x000))/(max(x000)-min(x000))\n",
    "nx110=(x110)#-min(x110))/(max(x110)-min(x110))\n",
    "nx220=(x220)#-min(x220))/(max(x220)-min(x220))\n",
    "nx330=(x330)#-min(x330))/(max(x330)-min(x330))\n",
    "nx440=(x440)#-min(x440))/(max(x440)-min(x440))\n",
    "nx550=(x550)#-min(x550))/(max(x550)-min(x550))\n",
    "nx660=(x660)#-min(x660))/(max(x660)-min(x660))\n",
    "nx770=(x770)#-min(x770))/(max(x770)-min(x770))\n",
    "nx880=(x880)#-min(x880))/(max(x880)-min(x880))\n",
    "nx990=(x990)#-min(x990))/(max(x990)-min(x990))\n",
    "ny990=(y990)#-min(y990))/(max(y990)-min(y990))\n",
    "\n",
    "\n",
    "z00=np.reshape(nx000,(1,314034))\n",
    "z10=np.reshape(nx110,(1,314034))\n",
    "z20=np.reshape(nx220,(1,314034))\n",
    "z30=np.reshape(nx330,(1,314034))\n",
    "z40=np.reshape(nx440,(1,314034))\n",
    "z50=np.reshape(nx550,(1,314034))\n",
    "z60=np.reshape(nx660,(1,314034))\n",
    "z70=np.reshape(nx770,(1,314034))\n",
    "z80=np.reshape(nx880,(1,314034))\n",
    "zx90=np.reshape(nx990,(1,314034))\n",
    "zy90=np.reshape(ny990,(1,314034))\n",
    "\n",
    "\n",
    "X0 = np.vstack((z00,z10,z20,z30,z40,z50,z60,z70,z80,zx90,zy90))\n",
    "Y0=np.zeros((1,X0.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#For Background 0 test\n",
    "mask00_test= (-1< deltatchod0_test) & (deltatchod0_test<1)\n",
    "x00a_test=deltatchod0_test[mask00_test]\n",
    "mask10_test= (-1< deltatrich0_test) & (deltatrich0_test<1)\n",
    "x10_test=deltatrich0_test[mask10_test]\n",
    "mask20_test= (-1< deltatktag0_test) & (deltatktag0_test<1)\n",
    "x20a_test=deltatktag0_test[mask20_test]\n",
    "mask30a_test= (cda0_test<15)\n",
    "x30a_test=cda0_test[mask30a_test]\n",
    "\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#'' x00a_test.shape                                                                    ''\n",
    "#'' x10_test.shape                                                                     ''\n",
    "#'' x20a_test.shape                                                                    ''\n",
    "#'' x30a_test.shape#to check the dimentionality then take the lowest dim mask for all  ''\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "x00_test=deltatchod0_test[mask10_test]\n",
    "x20_test=deltatktag0_test[mask10_test]# to fix the cut having the same shape\n",
    "x30_test=cda0_test[mask10_test]\n",
    "mask30_test=(x30_test<15)#fix the condition for mask30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x330_test=x30_test[mask30_test]#if lower dim apply it again for all\n",
    "x220_test=x20_test[mask30_test]\n",
    "x110_test=x10_test[mask30_test]\n",
    "x000_test=x00_test[mask30_test]\n",
    "#continue the same for all the other variables to unify the dimensions\n",
    "x40_test=chi20_test[mask10_test]\n",
    "x440_test=x40_test[mask30_test]\n",
    "x50_test=chi2p0_test[mask10_test]\n",
    "x550_test=x50_test[mask30_test]\n",
    "x60_test=lamd0_test[mask10_test]\n",
    "x660_test=x60_test[mask30_test]\n",
    "x70_test=dx0_test[mask10_test]\n",
    "x770_test=x70_test[mask30_test]\n",
    "x80_test=dy0_test[mask10_test]\n",
    "x880_test=x80_test[mask30_test]\n",
    "x90_test=vtrx0_test[mask10_test]\n",
    "x990_test=x90_test[mask30_test]\n",
    "y90_test=vtry0_test[mask10_test]\n",
    "y990_test=y90_test[mask30_test]\n",
    "\n",
    "nx000_test=(x000_test)#-min(x000_test))/(max(x000_test)-min(x000_test))\n",
    "nx110_test=(x110_test)#-min(x110_test))/(max(x110_test)-min(x110_test))\n",
    "nx220_test=(x220_test)#-min(x220_test))/(max(x220_test)-min(x220_test))\n",
    "nx330_test=(x330_test)#-min(x330_test))/(max(x330_test)-min(x330_test))\n",
    "nx440_test=(x440_test)#-min(x440_test))/(max(x440_test)-min(x440_test))\n",
    "nx550_test=(x550_test)#-min(x550_test))/(max(x550_test)-min(x550_test))\n",
    "nx660_test=(x660_test)#-min(x660_test))/(max(x660_test)-min(x660_test))\n",
    "nx770_test=(x770_test)#-min(x770_test))/(max(x770_test)-min(x770_test))\n",
    "nx880_test=(x880_test)#-min(x880_test))/(max(x880_test)-min(x880_test))\n",
    "nx990_test=(x990_test)#-min(x990_test))/(max(x990_test)-min(x990_test))\n",
    "ny990_test=(y990_test)#-min(y990_test))/(max(y990_test)-min(y990_test))\n",
    "\n",
    "\n",
    "z00_test=np.reshape(nx000_test,(1,35001))\n",
    "z10_test=np.reshape(nx110_test,(1,35001))\n",
    "z20_test=np.reshape(nx220_test,(1,35001))\n",
    "z30_test=np.reshape(nx330_test,(1,35001))\n",
    "z40_test=np.reshape(nx440_test,(1,35001))\n",
    "z50_test=np.reshape(nx550_test,(1,35001))\n",
    "z60_test=np.reshape(nx660_test,(1,35001))\n",
    "z70_test=np.reshape(nx770_test,(1,35001))\n",
    "z80_test=np.reshape(nx880_test,(1,35001))\n",
    "zx90_test=np.reshape(nx990_test,(1,35001))\n",
    "zy90_test=np.reshape(ny990_test,(1,35001))\n",
    "\n",
    "\n",
    "X0_test = np.vstack((z00_test,z10_test,z20_test,z30_test,z40_test,z50_test,z60_test,z70_test,z80_test,zx90_test,zy90_test))\n",
    "Y0_test=np.zeros((1,X0_test.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#For Signal 1 train\n",
    "mask01= (-1< deltatchod1) & (deltatchod1<1)\n",
    "x01a=deltatchod1[mask01]\n",
    "mask11= (-1< deltatrich1) & (deltatrich1<1)\n",
    "x11=deltatrich1[mask11]\n",
    "mask21= (-1< deltatktag1) & (deltatktag1<1)\n",
    "x21a=deltatktag1[mask21]\n",
    "mask31a= (cda1<15)\n",
    "x31a=cda1[mask31a]\n",
    "\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#'' x01a.shape                                                                    ''\n",
    "#'' x11.shape                                                                     ''\n",
    "#'' x21a.shape                                                                    ''\n",
    "#'' x31a.shape#to check the dimentionality then take the lowest dim mask for all  ''\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "x01=deltatchod1[mask11]\n",
    "x21=deltatktag1[mask11]# to fix the cut having the same shape\n",
    "x31=cda1[mask11]\n",
    "mask31=(x31<15)#fix the condition for mask30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x331=x31[mask31]#if lower dim apply it again for all\n",
    "x221=x21[mask31]\n",
    "x111=x11[mask31]\n",
    "x001=x01[mask31]\n",
    "#continue the same for all the other variables to unify the dimensions\n",
    "x41=chi21[mask11]\n",
    "x441=x41[mask31]\n",
    "x51=chi2p1[mask11]\n",
    "x551=x51[mask31]\n",
    "x61=lamd1[mask11]\n",
    "x661=x61[mask31]\n",
    "x71=dx1[mask11]\n",
    "x771=x71[mask31]\n",
    "x81=dy1[mask11]\n",
    "x881=x81[mask31]\n",
    "x91=vtrx1[mask11]\n",
    "x991=x91[mask31]\n",
    "y91=vtry1[mask11]\n",
    "y991=y91[mask31]\n",
    "\n",
    "nx001=(x001)#-min(x001))/(max(x001)-min(x001))\n",
    "nx111=(x111)#-min(x111))/(max(x111)-min(x111))\n",
    "nx221=(x221)#-min(x221))/(max(x221)-min(x221))\n",
    "nx331=(x331)#-min(x331))/(max(x331)-min(x331))\n",
    "nx441=(x441)#-min(x441))/(max(x441)-min(x441))\n",
    "nx551=(x551)#-min(x551))/(max(x551)-min(x551))\n",
    "nx661=(x661)#-min(x661))/(max(x661)-min(x661))\n",
    "nx771=(x771)#-min(x771))/(max(x771)-min(x771))\n",
    "nx881=(x881)#-min(x881))/(max(x881)-min(x881))\n",
    "nx991=(x991)#-min(x991))/(max(x991)-min(x991))\n",
    "ny991=(y991)#-min(y991))/(max(y991)-min(y991))\n",
    "\n",
    "\n",
    "z01=np.reshape(nx001,(1,417049))\n",
    "z11=np.reshape(nx111,(1,417049))\n",
    "z21=np.reshape(nx221,(1,417049))\n",
    "z31=np.reshape(nx331,(1,417049))\n",
    "z41=np.reshape(nx441,(1,417049))\n",
    "z51=np.reshape(nx551,(1,417049))\n",
    "z61=np.reshape(nx661,(1,417049))\n",
    "z71=np.reshape(nx771,(1,417049))\n",
    "z81=np.reshape(nx881,(1,417049))\n",
    "zx91=np.reshape(nx991,(1,417049))\n",
    "zy91=np.reshape(ny991,(1,417049))\n",
    "\n",
    "\n",
    "X1 = np.vstack((z01,z11,z21,z31,z41,z51,z61,z71,z81,zx91,zy91))\n",
    "Y1=np.ones((1,X1.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#For Signal 1 test\n",
    "mask01_test= (-1< deltatchod1_test) & (deltatchod1_test<1)\n",
    "x01a_test=deltatchod1_test[mask01_test]\n",
    "mask11_test= (-1< deltatrich1_test) & (deltatrich1_test<1)\n",
    "x11_test=deltatrich1_test[mask11_test]\n",
    "mask21_test= (-1< deltatktag1_test) & (deltatktag1_test<1)\n",
    "x21a_test=deltatktag1_test[mask21_test]\n",
    "mask31a_test= (cda1_test<15)\n",
    "x31a_test=cda1_test[mask31a_test]\n",
    "\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#'' x01a_test.shape                                                                    ''\n",
    "#'' x11_test.shape                                                                     ''\n",
    "#'' x21a_test.shape                                                                    ''\n",
    "#'' x31a_test.shape#to check the dimentionality then take the lowest dim mask for all  ''\n",
    "#'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "x01_test=deltatchod1_test[mask11_test]\n",
    "x21_test=deltatktag1_test[mask11_test]# to fix the cut having the same shape\n",
    "x31_test=cda1_test[mask11_test]\n",
    "mask31_test=(x31_test<15)#fix the condition for mask30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x331_test=x31_test[mask31_test]#if lower dim apply it again for all\n",
    "x221_test=x21_test[mask31_test]\n",
    "x111_test=x11_test[mask31_test]\n",
    "x001_test=x01_test[mask31_test]\n",
    "#continue the same for all the other variables to unify the dimensions\n",
    "x41_test=chi21_test[mask11_test]\n",
    "x441_test=x41_test[mask31_test]\n",
    "x51_test=chi2p1_test[mask11_test]\n",
    "x551_test=x51_test[mask31_test]\n",
    "x61_test=lamd1_test[mask11_test]\n",
    "x661_test=x61_test[mask31_test]\n",
    "x71_test=dx1_test[mask11_test]\n",
    "x771_test=x71_test[mask31_test]\n",
    "x81_test=dy1_test[mask11_test]\n",
    "x881_test=x81_test[mask31_test]\n",
    "x91_test=vtrx1_test[mask11_test]\n",
    "x991_test=x91_test[mask31_test]\n",
    "y91_test=vtry1_test[mask11_test]\n",
    "y991_test=y91_test[mask31_test]\n",
    "\n",
    "\n",
    "nx001_test=(x001_test)#-min(x001_test))/(max(x001_test)-min(x001_test))\n",
    "nx111_test=(x111_test)#-min(x111_test))/(max(x111_test)-min(x111_test))\n",
    "nx221_test=(x221_test)#-min(x221_test))/(max(x221_test)-min(x221_test))\n",
    "nx331_test=(x331_test)#-min(x331_test))/(max(x331_test)-min(x331_test))\n",
    "nx441_test=(x441_test)#-min(x441_test))/(max(x441_test)-min(x441_test))\n",
    "nx551_test=(x551_test)#-min(x551_test))/(max(x551_test)-min(x551_test))\n",
    "nx661_test=(x661_test)#-min(x661_test))/(max(x661_test)-min(x661_test))\n",
    "nx771_test=(x771_test)#-min(x771_test))/(max(x771_test)-min(x771_test))\n",
    "nx881_test=(x881_test)#-min(x881_test))/(max(x881_test)-min(x881_test))\n",
    "nx991_test=(x991_test)#-min(x991_test))/(max(x991_test)-min(x991_test))\n",
    "ny991_test=(y991_test)#-min(y991_test))/(max(y991_test)-min(y991_test))\n",
    "\n",
    "\n",
    "z01_test=np.reshape(nx001_test,(1,46359))\n",
    "z11_test=np.reshape(nx111_test,(1,46359))\n",
    "z21_test=np.reshape(x221_test,(1,46359))\n",
    "z31_test=np.reshape(nx331_test,(1,46359))\n",
    "z41_test=np.reshape(nx441_test,(1,46359))\n",
    "z51_test=np.reshape(nx551_test,(1,46359))\n",
    "z61_test=np.reshape(nx661_test,(1,46359))\n",
    "z71_test=np.reshape(nx771_test,(1,46359))\n",
    "z81_test=np.reshape(nx881_test,(1,46359))\n",
    "zx91_test=np.reshape(nx991_test,(1,46359))\n",
    "zy91_test=np.reshape(ny991_test,(1,46359))\n",
    "\n",
    "\n",
    "X1_test = np.vstack((z01_test,z11_test,z21_test,z31_test,z41_test,z51_test,z61_test,z71_test,z81_test,zx91_test,zy91_test))\n",
    "Y1_test = np.ones((1,X1_test.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "#input data X/output tag Y\n",
    "\n",
    "X=np.hstack((X0,X1))\n",
    "Y=np.hstack((Y0,Y1))\n",
    "\n",
    "X_test=np.hstack((X0_test,X1_test))\n",
    "Y_test=np.hstack((Y0_test,Y1_test))\n",
    "\n",
    "\n",
    "#X.shape\n",
    "#Y.shape\n",
    "#X_test.shape\n",
    "#Y_test.shape\n",
    "\n",
    "col_idx = np.random.permutation(X.shape[1])\n",
    "shuffled_X = X[:,col_idx]\n",
    "shuffled_Y = Y[:,col_idx]\n",
    "\n",
    "col_idx_test= np.random.permutation(X_test.shape[1])\n",
    "shuffled_X_test = X_test[:,col_idx_test]\n",
    "shuffled_Y_test = Y_test[:,col_idx_test]\n",
    "\n",
    "\n",
    "%matplotlib  inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "\n",
    "def model(X, Y, layers_dims, learning_rate = 1e-7, mini_batch_size = 256, beta1 = 0.9, beta2 = 0.99, epsilon = 1e-8, num_epochs = 7700, print_cost =True):\n",
    "    \n",
    "    L = len(layers_dims) # number of layers in the neural networks\n",
    "    costs = [] # to keep track of the cost\n",
    "    t = 0\n",
    "    seed = 10\n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    v, s = initialize_adam(parameters)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        for minibatch in minibatches:\n",
    "# Select a minibatch\n",
    "           (minibatch_X, minibatch_Y) = minibatch\n",
    "# Forward propagation\n",
    "           a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "# Compute cost\n",
    "           cost = compute_cost(a3, minibatch_Y)\n",
    "# Backward propagation\n",
    "           grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "           t = t + 1 # Adam counter\n",
    "           parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,t, learning_rate, beta1, beta2, epsilon)\n",
    "           \n",
    "    # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 100 == 0 :\n",
    "           print (\"Cost after epoch %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10 == 0 :\n",
    "           costs.append(cost)\n",
    "       \n",
    "       \n",
    " # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "    return parameters   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/na62/jcarmignani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:174: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.278410\n",
      "Cost after epoch 100: 0.412061\n",
      "Cost after epoch 200: 0.280450\n",
      "Cost after epoch 300: 0.278143\n",
      "Cost after epoch 400: 0.235836\n",
      "Cost after epoch 500: 0.236739\n",
      "Cost after epoch 600: 0.195096\n",
      "Cost after epoch 700: 0.136175\n",
      "Cost after epoch 800: 0.176366\n",
      "Cost after epoch 900: 0.228232\n",
      "Cost after epoch 1000: 0.116108\n",
      "Cost after epoch 1100: 0.156578\n",
      "Cost after epoch 1200: 0.242175\n",
      "Cost after epoch 1300: 0.149127\n",
      "Cost after epoch 1400: 0.214273\n",
      "Cost after epoch 1500: 0.160002\n",
      "Cost after epoch 1600: 0.134116\n",
      "Cost after epoch 1700: 0.103732\n",
      "Cost after epoch 1800: 0.110723\n",
      "Cost after epoch 1900: 0.196702\n",
      "Cost after epoch 2000: 0.226402\n",
      "Cost after epoch 2100: 0.120852\n",
      "Cost after epoch 2200: 0.180178\n",
      "Cost after epoch 2300: 0.102349\n",
      "Cost after epoch 2400: 0.161362\n",
      "Cost after epoch 2500: 0.211793\n",
      "Cost after epoch 2600: 0.133789\n",
      "Cost after epoch 2700: 0.140344\n",
      "Cost after epoch 2800: 0.141752\n",
      "Cost after epoch 2900: 0.210617\n",
      "Cost after epoch 3000: 0.158194\n",
      "Cost after epoch 3100: 0.152942\n",
      "Cost after epoch 3200: 0.171919\n",
      "Cost after epoch 3300: 0.148721\n",
      "Cost after epoch 3400: 0.134998\n",
      "Cost after epoch 3500: 0.174326\n",
      "Cost after epoch 3600: 0.084836\n",
      "Cost after epoch 3700: 0.139053\n",
      "Cost after epoch 3800: 0.178811\n",
      "Cost after epoch 3900: 0.159586\n",
      "Cost after epoch 4000: 0.149127\n",
      "Cost after epoch 4100: 0.145841\n",
      "Cost after epoch 4200: 0.224961\n",
      "Cost after epoch 4300: 0.112699\n",
      "Cost after epoch 4400: 0.117592\n",
      "Cost after epoch 4500: 0.116863\n",
      "Cost after epoch 4600: 0.093596\n",
      "Cost after epoch 4700: 0.169404\n",
      "Cost after epoch 4800: 0.086471\n",
      "Cost after epoch 4900: 0.182329\n",
      "Cost after epoch 5000: 0.072127\n",
      "Cost after epoch 5100: 0.193618\n",
      "Cost after epoch 5200: 0.149675\n",
      "Cost after epoch 5300: 0.100763\n",
      "Cost after epoch 5400: 0.085196\n",
      "Cost after epoch 5500: 0.287262\n",
      "Cost after epoch 5600: 0.158618\n",
      "Cost after epoch 5700: 0.092082\n",
      "Cost after epoch 5800: 0.177160\n",
      "Cost after epoch 5900: 0.161406\n",
      "Cost after epoch 6000: 0.172967\n",
      "Cost after epoch 6100: 0.145290\n",
      "Cost after epoch 6200: 0.150240\n",
      "Cost after epoch 6300: 0.069143\n",
      "Cost after epoch 6400: 0.174333\n",
      "Cost after epoch 6500: 0.171087\n",
      "Cost after epoch 6600: 0.153895\n",
      "Cost after epoch 6700: 0.148232\n",
      "Cost after epoch 6800: 0.121192\n",
      "Cost after epoch 6900: 0.128047\n",
      "Cost after epoch 7000: 0.107430\n",
      "Cost after epoch 7100: 0.114908\n",
      "Cost after epoch 7200: 0.100654\n",
      "Cost after epoch 7300: 0.109713\n",
      "Cost after epoch 7400: 0.148045\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------------------------\n",
    "# train 3-layer model\n",
    "layers_dims = [shuffled_X.shape[0], 530, 200, 1]\n",
    "parameters_v6 = model(shuffled_X, shuffled_Y, layers_dims)\n",
    "%store parameters_v6\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the training set:\n",
      "Accuracy: 0.938440642170588\n"
     ]
    }
   ],
   "source": [
    "#Call the parameters of the trained model\n",
    "%store -r parameters_v6\n",
    "#Now all you need is Predict(...)\n",
    "print (\"On the training set:\")\n",
    "predictions_training = predict(shuffled_X, shuffled_Y, parameters_v6)\n",
    "#two files will be automatically written for Kaon and Pile up misidentification...\n",
    "#Dont forget to change the names to avoid overwriting when repeating the procedure for the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the test set:\n",
      "Accuracy: 0.938901179941003\n"
     ]
    }
   ],
   "source": [
    "print (\"On the test set:\")\n",
    "predictions_testing = predict(shuffled_X_test, shuffled_Y_test, parameters_v6)\n",
    "#Save the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
